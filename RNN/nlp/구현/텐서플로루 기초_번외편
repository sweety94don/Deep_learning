@@ -0,0 +1,2259 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "작성자 : 노혜미 박승리 (YBIGTA 10기)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# (번외 1-1) Tensorflow를 활용한 Multi-Variable linear regression"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "import tensorflow as tf\n",
+    "import numpy as np\n",
+    "import time # 계산시간 측정"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "tf.set_random_seed(11)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "x1_data = [73., 93., 89., 96., 73.]\n",
+    "x2_data = [80., 88., 91., 98., 66.]\n",
+    "x3_data = [75., 93., 90., 100., 70.]\n",
+    "y_data = [152., 185., 180., 196., 142.] # float형태를 위해, 소수점을 표시"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "x1 = tf.placeholder(tf.float32)\n",
+    "x2 = tf.placeholder(tf.float32)\n",
+    "x3 = tf.placeholder(tf.float32)\n",
+    "Y = tf.placeholder(tf.float32)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "w1 = tf.Variable(tf.random_normal([1]), name = 'weight1')\n",
+    "w2 = tf.Variable(tf.random_normal([1]), name = 'weight2')\n",
+    "w3 = tf.Variable(tf.random_normal([1]), name = 'weight3')\n",
+    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
+    "\n",
+    "hypothesis = w1 * x1 + w2 * x2 + w3 * x3 + b"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# cost/Loss function\n",
+    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
+    "# Minimize function\n",
+    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 9,
+   "metadata": {
+    "scrolled": true
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 w1: [ 0.42367423] w2: [ 0.52396679] w3: [-0.23999393] b: [-0.4481698] cost: 40421.7\n",
+      "step: 100 w1: [ 0.86809784] w2: [ 0.94971919] w3: [ 0.20391375] b: [-0.44303927] cost: 3.34129\n",
+      "step: 200 w1: [ 0.87581116] w2: [ 0.93968588] w3: [ 0.20619662] b: [-0.44298562] cost: 3.1758\n",
+      "step: 300 w1: [ 0.88331354] w2: [ 0.92991853] w3: [ 0.20842525] b: [-0.44293383] cost: 3.0191\n",
+      "step: 400 w1: [ 0.89061087] w2: [ 0.9204098] w3: [ 0.21060142] b: [-0.44288316] cost: 2.87061\n",
+      "step: 500 w1: [ 0.89770812] w2: [ 0.9111532] w3: [ 0.21272619] b: [-0.44283393] cost: 2.72998\n",
+      "step: 600 w1: [ 0.90461117] w2: [ 0.90214139] w3: [ 0.21480125] b: [-0.44278625] cost: 2.59675\n",
+      "step: 700 w1: [ 0.91132486] w2: [ 0.89336836] w3: [ 0.21682774] b: [-0.44273913] cost: 2.47052\n",
+      "step: 800 w1: [ 0.91785461] w2: [ 0.88482732] w3: [ 0.21880697] b: [-0.44269443] cost: 2.35098\n",
+      "step: 900 w1: [ 0.92420495] w2: [ 0.87651259] w3: [ 0.22074017] b: [-0.44264972] cost: 2.23771\n",
+      "step: 1000 w1: [ 0.93038076] w2: [ 0.8684178] w3: [ 0.22262859] b: [-0.44260722] cost: 2.13042\n",
+      "step: 1100 w1: [ 0.9363867] w2: [ 0.86053717] w3: [ 0.22447343] b: [-0.4425655] cost: 2.02881\n",
+      "step: 1200 w1: [ 0.9422276] w2: [ 0.85286492] w3: [ 0.2262757] b: [-0.44252411] cost: 1.93253\n",
+      "step: 1300 w1: [ 0.94790775] w2: [ 0.84539545] w3: [ 0.22803673] b: [-0.44248536] cost: 1.84134\n",
+      "step: 1400 w1: [ 0.95343161] w2: [ 0.83812326] w3: [ 0.22975726] b: [-0.44244662] cost: 1.75491\n",
+      "step: 1500 w1: [ 0.95880306] w2: [ 0.83104324] w3: [ 0.23143885] b: [-0.44240868] cost: 1.67308\n",
+      "step: 1600 w1: [ 0.96402609] w2: [ 0.82415026] w3: [ 0.23308226] b: [-0.44237292] cost: 1.59554\n",
+      "step: 1700 w1: [ 0.96910518] w2: [ 0.8174392] w3: [ 0.23468854] b: [-0.44233716] cost: 1.52209\n",
+      "step: 1800 w1: [ 0.97404397] w2: [ 0.81090516] w3: [ 0.23625866] b: [-0.4423019] cost: 1.45251\n",
+      "step: 1900 w1: [ 0.97884625] w2: [ 0.80454361] w3: [ 0.23779346] b: [-0.44226912] cost: 1.38659\n",
+      "step: 2000 w1: [ 0.98351532] w2: [ 0.79834968] w3: [ 0.23929411] b: [-0.44223633] cost: 1.32414\n",
+      "step: 2100 w1: [ 0.98805499] w2: [ 0.79231948] w3: [ 0.24076122] b: [-0.44220355] cost: 1.26499\n",
+      "step: 2200 w1: [ 0.99246877] w2: [ 0.78644818] w3: [ 0.24219586] b: [-0.44217303] cost: 1.20897\n",
+      "step: 2300 w1: [ 0.99676013] w2: [ 0.78073156] w3: [ 0.24359879] b: [-0.44214323] cost: 1.15586\n",
+      "step: 2400 w1: [ 1.00093186] w2: [ 0.7751658] w3: [ 0.2449709] b: [-0.44211343] cost: 1.10556\n",
+      "step: 2500 w1: [ 1.00498772] w2: [ 0.76974636] w3: [ 0.24631318] b: [-0.44208363] cost: 1.05792\n",
+      "step: 2600 w1: [ 1.00893021] w2: [ 0.76447004] w3: [ 0.2476262] b: [-0.44205663] cost: 1.01279\n",
+      "step: 2700 w1: [ 1.01276338] w2: [ 0.75933206] w3: [ 0.24891061] b: [-0.4420298] cost: 0.970039\n",
+      "step: 2800 w1: [ 1.01648951] w2: [ 0.7543292] w3: [ 0.2501674] b: [-0.44200298] cost: 0.92951\n",
+      "step: 2900 w1: [ 1.02011132] w2: [ 0.74945831] w3: [ 0.25139707] b: [-0.44197616] cost: 0.891136\n",
+      "step: 3000 w1: [ 1.02363169] w2: [ 0.74471539] w3: [ 0.25260064] b: [-0.44195133] cost: 0.854791\n",
+      "step: 3100 w1: [ 1.02705371] w2: [ 0.74009681] w3: [ 0.25377855] b: [-0.44192749] cost: 0.820323\n",
+      "step: 3200 w1: [ 1.03037965] w2: [ 0.7355997] w3: [ 0.25493157] b: [-0.44190365] cost: 0.787693\n",
+      "step: 3300 w1: [ 1.03361249] w2: [ 0.73122054] w3: [ 0.25606012] b: [-0.44187981] cost: 0.756766\n",
+      "step: 3400 w1: [ 1.03675425] w2: [ 0.72695637] w3: [ 0.25716519] b: [-0.44185597] cost: 0.727491\n",
+      "step: 3500 w1: [ 1.0398078] w2: [ 0.72280401] w3: [ 0.25824699] b: [-0.44183436] cost: 0.699735\n",
+      "step: 3600 w1: [ 1.04277503] w2: [ 0.71876073] w3: [ 0.25930646] b: [-0.4418135] cost: 0.673431\n",
+      "step: 3700 w1: [ 1.04565895] w2: [ 0.71482313] w3: [ 0.26034388] b: [-0.44179264] cost: 0.648518\n",
+      "step: 3800 w1: [ 1.0484612] w2: [ 0.71098864] w3: [ 0.26136035] b: [-0.44177178] cost: 0.624927\n",
+      "step: 3900 w1: [ 1.05118418] w2: [ 0.70725465] w3: [ 0.2623558] b: [-0.44175091] cost: 0.60256\n",
+      "step: 4000 w1: [ 1.05383003] w2: [ 0.70361805] w3: [ 0.26333129] b: [-0.44173005] cost: 0.581372\n",
+      "step: 4100 w1: [ 1.05640125] w2: [ 0.70007646] w3: [ 0.26428685] b: [-0.44171205] cost: 0.561279\n",
+      "step: 4200 w1: [ 1.05889893] w2: [ 0.69662768] w3: [ 0.26522341] b: [-0.44169417] cost: 0.542276\n",
+      "step: 4300 w1: [ 1.06132579] w2: [ 0.69326872] w3: [ 0.26614144] b: [-0.44167629] cost: 0.52424\n",
+      "step: 4400 w1: [ 1.06368327] w2: [ 0.68999743] w3: [ 0.26704112] b: [-0.44165841] cost: 0.507163\n",
+      "step: 4500 w1: [ 1.06597352] w2: [ 0.68681145] w3: [ 0.26792309] b: [-0.44164053] cost: 0.490985\n",
+      "step: 4600 w1: [ 1.0681982] w2: [ 0.68370855] w3: [ 0.26878801] b: [-0.44162264] cost: 0.475646\n",
+      "step: 4700 w1: [ 1.07035947] w2: [ 0.68068653] w3: [ 0.26963589] b: [-0.44160476] cost: 0.461119\n",
+      "step: 4800 w1: [ 1.07245874] w2: [ 0.67774278] w3: [ 0.27046743] b: [-0.44158939] cost: 0.447344\n",
+      "step: 4900 w1: [ 1.07449746] w2: [ 0.67487603] w3: [ 0.271283] b: [-0.44157448] cost: 0.4343\n",
+      "step: 5000 w1: [ 1.07647729] w2: [ 0.67208385] w3: [ 0.27208316] b: [-0.44155958] cost: 0.421932\n",
+      "step: 5100 w1: [ 1.07840014] w2: [ 0.66936415] w3: [ 0.27286795] b: [-0.44154468] cost: 0.41022\n",
+      "step: 5200 w1: [ 1.08026731] w2: [ 0.66671509] w3: [ 0.27363792] b: [-0.44152978] cost: 0.399116\n",
+      "step: 5300 w1: [ 1.08208036] w2: [ 0.66413456] w3: [ 0.27439395] b: [-0.44151488] cost: 0.38859\n",
+      "step: 5400 w1: [ 1.08384097] w2: [ 0.66162103] w3: [ 0.27513561] b: [-0.44149998] cost: 0.378617\n",
+      "step: 5500 w1: [ 1.08554983] w2: [ 0.65917277] w3: [ 0.27586377] b: [-0.44148508] cost: 0.369167\n",
+      "step: 5600 w1: [ 1.08720911] w2: [ 0.65678793] w3: [ 0.27657855] b: [-0.44147018] cost: 0.360213\n",
+      "step: 5700 w1: [ 1.08881974] w2: [ 0.65446442] w3: [ 0.27728069] b: [-0.44145727] cost: 0.351724\n",
+      "step: 5800 w1: [ 1.09038317] w2: [ 0.65220129] w3: [ 0.27797011] b: [-0.44144535] cost: 0.343668\n",
+      "step: 5900 w1: [ 1.09190106] w2: [ 0.6499964] w3: [ 0.27864695] b: [-0.44143343] cost: 0.336045\n",
+      "step: 6000 w1: [ 1.09337401] w2: [ 0.64784849] w3: [ 0.27931195] b: [-0.44142151] cost: 0.328809\n",
+      "step: 6100 w1: [ 1.09480321] w2: [ 0.64575589] w3: [ 0.27996537] b: [-0.44140959] cost: 0.321964\n",
+      "step: 6200 w1: [ 1.09619081] w2: [ 0.64371711] w3: [ 0.28060707] b: [-0.44139767] cost: 0.315458\n",
+      "step: 6300 w1: [ 1.09753656] w2: [ 0.64173096] w3: [ 0.2812379] b: [-0.44138575] cost: 0.309302\n",
+      "step: 6400 w1: [ 1.09884238] w2: [ 0.6397959] w3: [ 0.28185791] b: [-0.44137383] cost: 0.303463\n",
+      "step: 6500 w1: [ 1.10010958] w2: [ 0.63791025] w3: [ 0.28246737] b: [-0.4413619] cost: 0.297932\n",
+      "step: 6600 w1: [ 1.1013391] w2: [ 0.63607293] w3: [ 0.28306621] b: [-0.44134998] cost: 0.292685\n",
+      "step: 6700 w1: [ 1.10253119] w2: [ 0.63428283] w3: [ 0.28365538] b: [-0.44133806] cost: 0.287702\n",
+      "step: 6800 w1: [ 1.10368752] w2: [ 0.63253868] w3: [ 0.28423461] b: [-0.44132614] cost: 0.282986\n",
+      "step: 6900 w1: [ 1.10480869] w2: [ 0.63083911] w3: [ 0.28480458] b: [-0.44131422] cost: 0.27852\n",
+      "step: 7000 w1: [ 1.10589588] w2: [ 0.62918293] w3: [ 0.28536522] b: [-0.44130301] cost: 0.274272\n",
+      "step: 7100 w1: [ 1.10695004] w2: [ 0.62756926] w3: [ 0.28591666] b: [-0.44129407] cost: 0.270257\n",
+      "step: 7200 w1: [ 1.10797215] w2: [ 0.62599641] w3: [ 0.28645939] b: [-0.44128513] cost: 0.266432\n",
+      "step: 7300 w1: [ 1.10896242] w2: [ 0.62446386] w3: [ 0.28699383] b: [-0.44127619] cost: 0.262814\n",
+      "step: 7400 w1: [ 1.10992241] w2: [ 0.62297028] w3: [ 0.28751963] b: [-0.44126725] cost: 0.259387\n",
+      "step: 7500 w1: [ 1.1108526] w2: [ 0.62151474] w3: [ 0.28803745] b: [-0.44125831] cost: 0.256134\n",
+      "step: 7600 w1: [ 1.11175418] w2: [ 0.62009609] w3: [ 0.2885471] b: [-0.44124937] cost: 0.253044\n",
+      "step: 7700 w1: [ 1.11262739] w2: [ 0.61871326] w3: [ 0.2890493] b: [-0.44124043] cost: 0.250122\n",
+      "step: 7800 w1: [ 1.11347377] w2: [ 0.61736548] w3: [ 0.28954369] b: [-0.44123149] cost: 0.24734\n",
+      "step: 7900 w1: [ 1.11429358] w2: [ 0.61605179] w3: [ 0.29003051] b: [-0.44122255] cost: 0.244704\n",
+      "step: 8000 w1: [ 1.11508763] w2: [ 0.61477107] w3: [ 0.29051018] b: [-0.44121361] cost: 0.242203\n",
+      "step: 8100 w1: [ 1.11585617] w2: [ 0.61352277] w3: [ 0.29098308] b: [-0.44120467] cost: 0.239824\n",
+      "step: 8200 w1: [ 1.11660051] w2: [ 0.6123057] w3: [ 0.29144892] b: [-0.44119573] cost: 0.237572\n",
+      "step: 8300 w1: [ 1.11732125] w2: [ 0.61111939] w3: [ 0.29190812] b: [-0.44118679] cost: 0.235431\n",
+      "step: 8400 w1: [ 1.11801851] w2: [ 0.6099627] w3: [ 0.29236093] b: [-0.44117785] cost: 0.233401\n",
+      "step: 8500 w1: [ 1.11869323] w2: [ 0.60883498] w3: [ 0.29280749] b: [-0.4411689] cost: 0.231481\n",
+      "step: 8600 w1: [ 1.11934566] w2: [ 0.60773587] w3: [ 0.29324788] b: [-0.44115996] cost: 0.229646\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 8700 w1: [ 1.11997724] w2: [ 0.60666394] w3: [ 0.29368183] b: [-0.44115102] cost: 0.227903\n",
+      "step: 8800 w1: [ 1.1205883] w2: [ 0.6056186] w3: [ 0.29411012] b: [-0.44114208] cost: 0.226254\n",
+      "step: 8900 w1: [ 1.12117875] w2: [ 0.60459971] w3: [ 0.29453257] b: [-0.44113314] cost: 0.224689\n",
+      "step: 9000 w1: [ 1.1217494] w2: [ 0.60360587] w3: [ 0.29494974] b: [-0.4411242] cost: 0.223199\n",
+      "step: 9100 w1: [ 1.12230206] w2: [ 0.60263628] w3: [ 0.29536089] b: [-0.44111526] cost: 0.221782\n",
+      "step: 9200 w1: [ 1.1228348] w2: [ 0.60169113] w3: [ 0.29576731] b: [-0.44110632] cost: 0.22045\n",
+      "step: 9300 w1: [ 1.1233505] w2: [ 0.6007691] w3: [ 0.29616809] b: [-0.44109738] cost: 0.219171\n",
+      "step: 9400 w1: [ 1.1238476] w2: [ 0.59987026] w3: [ 0.29656413] b: [-0.44108844] cost: 0.217955\n",
+      "step: 9500 w1: [ 1.12432861] w2: [ 0.59899312] w3: [ 0.29695502] b: [-0.4410795] cost: 0.216796\n",
+      "step: 9600 w1: [ 1.12479234] w2: [ 0.5981378] w3: [ 0.29734093] b: [-0.44107056] cost: 0.215707\n",
+      "step: 9700 w1: [ 1.12523985] w2: [ 0.59730345] w3: [ 0.29772243] b: [-0.44106162] cost: 0.214657\n",
+      "step: 9800 w1: [ 1.12567222] w2: [ 0.59648943] w3: [ 0.29809892] b: [-0.44105268] cost: 0.213669\n",
+      "step: 9900 w1: [ 1.12608874] w2: [ 0.59569538] w3: [ 0.298471] b: [-0.44104373] cost: 0.212728\n",
+      "step: 10000 w1: [ 1.12649] w2: [ 0.59492105] w3: [ 0.298839] b: [-0.44103479] cost: 0.211829\n",
+      "step: 10100 w1: [ 1.12687731] w2: [ 0.59416515] w3: [ 0.29920256] b: [-0.44102633] cost: 0.210975\n",
+      "step: 10200 w1: [ 1.12725079] w2: [ 0.59342766] w3: [ 0.29956168] b: [-0.44102037] cost: 0.210168\n",
+      "step: 10300 w1: [ 1.12761045] w2: [ 0.59270775] w3: [ 0.2999168] b: [-0.44101441] cost: 0.209389\n",
+      "step: 10400 w1: [ 1.12795675] w2: [ 0.59200543] w3: [ 0.30026808] b: [-0.44100845] cost: 0.208653\n",
+      "step: 10500 w1: [ 1.12829006] w2: [ 0.59131998] w3: [ 0.30061546] b: [-0.44100249] cost: 0.207949\n",
+      "step: 10600 w1: [ 1.12861073] w2: [ 0.59065127] w3: [ 0.30095863] b: [-0.44099653] cost: 0.207278\n",
+      "step: 10700 w1: [ 1.12891924] w2: [ 0.5899983] w3: [ 0.3012985] b: [-0.44099057] cost: 0.206638\n",
+      "step: 10800 w1: [ 1.12921584] w2: [ 0.58936095] w3: [ 0.30163455] b: [-0.44098461] cost: 0.206035\n",
+      "step: 10900 w1: [ 1.12950075] w2: [ 0.58873904] w3: [ 0.30196711] b: [-0.44097865] cost: 0.205454\n",
+      "step: 11000 w1: [ 1.12977445] w2: [ 0.58813167] w3: [ 0.30229631] b: [-0.44097269] cost: 0.204905\n",
+      "step: 11100 w1: [ 1.13003719] w2: [ 0.58753884] w3: [ 0.30262196] b: [-0.44096673] cost: 0.204377\n",
+      "step: 11200 w1: [ 1.13028896] w2: [ 0.58696014] w3: [ 0.30294454] b: [-0.44096076] cost: 0.203882\n",
+      "step: 11300 w1: [ 1.13053024] w2: [ 0.5863952] w3: [ 0.30326369] b: [-0.4409548] cost: 0.203393\n",
+      "step: 11400 w1: [ 1.13076162] w2: [ 0.58584338] w3: [ 0.3035799] b: [-0.44094884] cost: 0.202938\n",
+      "step: 11500 w1: [ 1.13098359] w2: [ 0.58530462] w3: [ 0.30389252] b: [-0.44094288] cost: 0.202497\n",
+      "step: 11600 w1: [ 1.13119638] w2: [ 0.58477777] w3: [ 0.30420223] b: [-0.44093692] cost: 0.202079\n",
+      "step: 11700 w1: [ 1.13139951] w2: [ 0.58426386] w3: [ 0.30450913] b: [-0.44093096] cost: 0.201686\n",
+      "step: 11800 w1: [ 1.13159263] w2: [ 0.58376199] w3: [ 0.30481362] b: [-0.440925] cost: 0.201297\n",
+      "step: 11900 w1: [ 1.13177776] w2: [ 0.58327168] w3: [ 0.30511469] b: [-0.44091904] cost: 0.200934\n",
+      "step: 12000 w1: [ 1.13195515] w2: [ 0.5827921] w3: [ 0.30541286] b: [-0.44091308] cost: 0.200584\n",
+      "step: 12100 w1: [ 1.13212276] w2: [ 0.5823245] w3: [ 0.30570889] b: [-0.44090712] cost: 0.200249\n",
+      "step: 12200 w1: [ 1.13228273] w2: [ 0.58186716] w3: [ 0.30600226] b: [-0.44090116] cost: 0.199925\n",
+      "step: 12300 w1: [ 1.13243604] w2: [ 0.58142012] w3: [ 0.30629218] b: [-0.4408952] cost: 0.19962\n",
+      "step: 12400 w1: [ 1.13257992] w2: [ 0.58098382] w3: [ 0.30658075] b: [-0.44088924] cost: 0.199319\n",
+      "step: 12500 w1: [ 1.13271749] w2: [ 0.58055681] w3: [ 0.30686626] b: [-0.44088328] cost: 0.199039\n",
+      "step: 12600 w1: [ 1.13284838] w2: [ 0.58013946] w3: [ 0.30714899] b: [-0.44087732] cost: 0.19877\n",
+      "step: 12700 w1: [ 1.13297045] w2: [ 0.57973206] w3: [ 0.30743057] b: [-0.44087136] cost: 0.198513\n",
+      "step: 12800 w1: [ 1.13308811] w2: [ 0.57933301] w3: [ 0.30770832] b: [-0.4408654] cost: 0.198255\n",
+      "step: 12900 w1: [ 1.13319671] w2: [ 0.57894385] w3: [ 0.30798516] b: [-0.44085944] cost: 0.198017\n",
+      "step: 13000 w1: [ 1.13330138] w2: [ 0.57856226] w3: [ 0.3082585] b: [-0.44085348] cost: 0.197786\n",
+      "step: 13100 w1: [ 1.13339734] w2: [ 0.57819027] w3: [ 0.3085309] b: [-0.44084752] cost: 0.197566\n",
+      "step: 13200 w1: [ 1.13348973] w2: [ 0.57782495] w3: [ 0.30880019] b: [-0.44084156] cost: 0.197348\n",
+      "step: 13300 w1: [ 1.13357353] w2: [ 0.57746953] w3: [ 0.3090685] b: [-0.4408356] cost: 0.19714\n",
+      "step: 13400 w1: [ 1.13365412] w2: [ 0.5771203] w3: [ 0.30933338] b: [-0.44082963] cost: 0.196948\n",
+      "step: 13500 w1: [ 1.13372648] w2: [ 0.57677996] w3: [ 0.30959797] b: [-0.44082367] cost: 0.196761\n",
+      "step: 13600 w1: [ 1.13379645] w2: [ 0.5764457] w3: [ 0.30985877] b: [-0.44081771] cost: 0.196569\n",
+      "step: 13700 w1: [ 1.13385797] w2: [ 0.57612062] w3: [ 0.310119] b: [-0.44081175] cost: 0.196392\n",
+      "step: 13800 w1: [ 1.13391721] w2: [ 0.5758006] w3: [ 0.31037652] b: [-0.44080579] cost: 0.19622\n",
+      "step: 13900 w1: [ 1.13396966] w2: [ 0.57548833] w3: [ 0.31063277] b: [-0.44079983] cost: 0.196052\n",
+      "step: 14000 w1: [ 1.13401735] w2: [ 0.57518309] w3: [ 0.31088716] b: [-0.44079387] cost: 0.195893\n",
+      "step: 14100 w1: [ 1.13406253] w2: [ 0.57488275] w3: [ 0.31113887] b: [-0.44078788] cost: 0.195741\n",
+      "step: 14200 w1: [ 1.13409925] w2: [ 0.57459056] w3: [ 0.31139103] b: [-0.44078019] cost: 0.195585\n",
+      "step: 14300 w1: [ 1.13413477] w2: [ 0.57430357] w3: [ 0.31163925] b: [-0.44077212] cost: 0.195442\n",
+      "step: 14400 w1: [ 1.13416612] w2: [ 0.57402188] w3: [ 0.31188643] b: [-0.44076318] cost: 0.1953\n",
+      "step: 14500 w1: [ 1.13419008] w2: [ 0.57374775] w3: [ 0.31213322] b: [-0.44075423] cost: 0.195159\n",
+      "step: 14600 w1: [ 1.13421381] w2: [ 0.5734781] w3: [ 0.31237614] b: [-0.44074529] cost: 0.195031\n",
+      "step: 14700 w1: [ 1.13423216] w2: [ 0.57321388] w3: [ 0.31261855] b: [-0.44073635] cost: 0.194903\n",
+      "step: 14800 w1: [ 1.13424444] w2: [ 0.57295674] w3: [ 0.3128604] b: [-0.44072741] cost: 0.194779\n",
+      "step: 14900 w1: [ 1.13425624] w2: [ 0.57270259] w3: [ 0.31309962] b: [-0.44071847] cost: 0.194658\n",
+      "step: 15000 w1: [ 1.13426423] w2: [ 0.57245398] w3: [ 0.313337] b: [-0.44070953] cost: 0.194538\n",
+      "step: 15100 w1: [ 1.1342659] w2: [ 0.5722118] w3: [ 0.31357455] b: [-0.44070059] cost: 0.194422\n",
+      "step: 15200 w1: [ 1.1342659] w2: [ 0.57197344] w3: [ 0.31380978] b: [-0.44069165] cost: 0.194312\n",
+      "step: 15300 w1: [ 1.13426471] w2: [ 0.57173914] w3: [ 0.31404242] b: [-0.44068271] cost: 0.194198\n",
+      "step: 15400 w1: [ 1.13425779] w2: [ 0.57151031] w3: [ 0.31427488] b: [-0.44067377] cost: 0.194095\n",
+      "step: 15500 w1: [ 1.13424659] w2: [ 0.57128662] w3: [ 0.3145068] b: [-0.44066483] cost: 0.193992\n",
+      "step: 15600 w1: [ 1.13423467] w2: [ 0.5710662] w3: [ 0.31473625] b: [-0.44065589] cost: 0.19389\n",
+      "step: 15700 w1: [ 1.1342212] w2: [ 0.57084936] w3: [ 0.31496352] b: [-0.44064695] cost: 0.19379\n",
+      "step: 15800 w1: [ 1.1342026] w2: [ 0.5706377] w3: [ 0.31519079] b: [-0.44063801] cost: 0.193697\n",
+      "step: 15900 w1: [ 1.13417923] w2: [ 0.57043099] w3: [ 0.31541789] b: [-0.44062907] cost: 0.1936\n",
+      "step: 16000 w1: [ 1.13415539] w2: [ 0.57022792] w3: [ 0.31564197] b: [-0.44062012] cost: 0.19351\n",
+      "step: 16100 w1: [ 1.13413119] w2: [ 0.57002646] w3: [ 0.31586474] b: [-0.44061118] cost: 0.193413\n",
+      "step: 16200 w1: [ 1.13410306] w2: [ 0.56982982] w3: [ 0.31608656] b: [-0.44060224] cost: 0.193328\n",
+      "step: 16300 w1: [ 1.1340698] w2: [ 0.56963825] w3: [ 0.31630853] b: [-0.4405933] cost: 0.193241\n",
+      "step: 16400 w1: [ 1.13403404] w2: [ 0.56945008] w3: [ 0.31652966] b: [-0.44058436] cost: 0.193155\n",
+      "step: 16500 w1: [ 1.13399827] w2: [ 0.56926495] w3: [ 0.31674778] b: [-0.44057542] cost: 0.193073\n",
+      "step: 16600 w1: [ 1.13396239] w2: [ 0.56908101] w3: [ 0.31696481] b: [-0.44056648] cost: 0.19299\n",
+      "step: 16700 w1: [ 1.13392282] w2: [ 0.56890172] w3: [ 0.31718066] b: [-0.44055754] cost: 0.192905\n",
+      "step: 16800 w1: [ 1.13387907] w2: [ 0.56872678] w3: [ 0.31739667] b: [-0.4405486] cost: 0.192832\n",
+      "step: 16900 w1: [ 1.13383222] w2: [ 0.56855494] w3: [ 0.31761247] b: [-0.44053966] cost: 0.192749\n",
+      "step: 17000 w1: [ 1.13378453] w2: [ 0.56838644] w3: [ 0.31782606] b: [-0.44053072] cost: 0.192671\n",
+      "step: 17100 w1: [ 1.13373685] w2: [ 0.56821948] w3: [ 0.31803787] b: [-0.44052178] cost: 0.192599\n",
+      "step: 17200 w1: [ 1.13368893] w2: [ 0.56805396] w3: [ 0.31824866] b: [-0.44051284] cost: 0.192529\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 17300 w1: [ 1.13363814] w2: [ 0.56789261] w3: [ 0.31845814] b: [-0.4405039] cost: 0.192449\n",
+      "step: 17400 w1: [ 1.13358366] w2: [ 0.56773472] w3: [ 0.31866774] b: [-0.44049495] cost: 0.192382\n",
+      "step: 17500 w1: [ 1.13352609] w2: [ 0.56757993] w3: [ 0.31887734] b: [-0.44048601] cost: 0.192308\n",
+      "step: 17600 w1: [ 1.13346648] w2: [ 0.56742811] w3: [ 0.3190861] b: [-0.44047707] cost: 0.192238\n",
+      "step: 17700 w1: [ 1.13340688] w2: [ 0.56727892] w3: [ 0.31929237] b: [-0.44046813] cost: 0.19217\n",
+      "step: 17800 w1: [ 1.13334727] w2: [ 0.56713015] w3: [ 0.31949806] b: [-0.44045919] cost: 0.192098\n",
+      "step: 17900 w1: [ 1.13328767] w2: [ 0.5669831] w3: [ 0.31970218] b: [-0.44045025] cost: 0.192037\n",
+      "step: 18000 w1: [ 1.13322639] w2: [ 0.5668391] w3: [ 0.31990489] b: [-0.44044131] cost: 0.191971\n",
+      "step: 18100 w1: [ 1.13316274] w2: [ 0.56669712] w3: [ 0.3201077] b: [-0.44043237] cost: 0.191902\n",
+      "step: 18200 w1: [ 1.13309562] w2: [ 0.56655926] w3: [ 0.32031015] b: [-0.44042343] cost: 0.191838\n",
+      "step: 18300 w1: [ 1.13302648] w2: [ 0.5664233] w3: [ 0.32051253] b: [-0.44041449] cost: 0.191774\n",
+      "step: 18400 w1: [ 1.13295519] w2: [ 0.56629068] w3: [ 0.32071388] b: [-0.44040555] cost: 0.19171\n",
+      "step: 18500 w1: [ 1.13288367] w2: [ 0.56615955] w3: [ 0.320914] b: [-0.44039661] cost: 0.19165\n",
+      "step: 18600 w1: [ 1.13281214] w2: [ 0.56602877] w3: [ 0.32111377] b: [-0.44038767] cost: 0.191591\n",
+      "step: 18700 w1: [ 1.13274062] w2: [ 0.56589979] w3: [ 0.32131189] b: [-0.44037873] cost: 0.191527\n",
+      "step: 18800 w1: [ 1.13266897] w2: [ 0.56577289] w3: [ 0.32150796] b: [-0.44036978] cost: 0.191463\n",
+      "step: 18900 w1: [ 1.13259733] w2: [ 0.56564766] w3: [ 0.32170227] b: [-0.44036084] cost: 0.191405\n",
+      "step: 19000 w1: [ 1.13252473] w2: [ 0.56552309] w3: [ 0.32189691] b: [-0.4403519] cost: 0.191349\n",
+      "step: 19100 w1: [ 1.13244903] w2: [ 0.56540215] w3: [ 0.32209095] b: [-0.44034296] cost: 0.191288\n",
+      "step: 19200 w1: [ 1.1323719] w2: [ 0.56528294] w3: [ 0.32228491] b: [-0.44033402] cost: 0.191227\n",
+      "step: 19300 w1: [ 1.13229299] w2: [ 0.56516546] w3: [ 0.32247874] b: [-0.44032508] cost: 0.19118\n",
+      "step: 19400 w1: [ 1.13221097] w2: [ 0.56505156] w3: [ 0.32267231] b: [-0.44031614] cost: 0.191119\n",
+      "step: 19500 w1: [ 1.13212812] w2: [ 0.56493831] w3: [ 0.32286578] b: [-0.4403072] cost: 0.19106\n",
+      "step: 19600 w1: [ 1.13204479] w2: [ 0.56482601] w3: [ 0.32305896] b: [-0.44029826] cost: 0.191005\n",
+      "step: 19700 w1: [ 1.13196135] w2: [ 0.5647158] w3: [ 0.32325029] b: [-0.44028932] cost: 0.190949\n",
+      "step: 19800 w1: [ 1.1318779] w2: [ 0.56460702] w3: [ 0.3234401] b: [-0.44028038] cost: 0.190894\n",
+      "step: 19900 w1: [ 1.13179445] w2: [ 0.56449962] w3: [ 0.3236286] b: [-0.44027144] cost: 0.190841\n",
+      "step: 20000 w1: [ 1.13171101] w2: [ 0.56439233] w3: [ 0.32381684] b: [-0.4402625] cost: 0.190783\n",
+      "12.933414697647095\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Make Session\n",
+    "sess = tf.Session()\n",
+    "# Initialize Variables\n",
+    "sess.run(tf.global_variables_initializer())\n",
+    "\n",
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    \n",
+    "    t1 = time.time()\n",
+    "    \n",
+    "    for step in range(20001):\n",
+    "        cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
+    "    \n",
+    "        if step % 100 == 0:\n",
+    "            print('step:', step, 'w1:', sess.run(w1), 'w2:', sess.run(w2), 'w3:', sess.run(w3), 'b:', sess.run(b), 'cost:', cost_val) \n",
+    "    \n",
+    "    t2 = time.time()\n",
+    "    print(t2-t1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 10,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "InvalidArgumentError",
+     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-e108970238bc>\", line 1, in <module>\n    x1 = tf.placeholder(tf.float32)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1530, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1954, in _placeholder\n    name=name)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
+      "\nDuring handling of the above exception, another exception occurred:\n",
+      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
+      "\u001b[1;32m<ipython-input-10-99aad76c4c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'step:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w1:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w2:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w3:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cost:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
+      "\u001b[1;32mC:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-e108970238bc>\", line 1, in <module>\n    x1 = tf.placeholder(tf.float32)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1530, in placeholder\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1954, in _placeholder\n    name=name)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\PARK SEONGRI\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Make Session\n",
+    "sess = tf.Session()\n",
+    "# Initialize Variables\n",
+    "sess.run(tf.global_variables_initializer())\n",
+    "for step in range(20001):\n",
+    "    sess.run(train, feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, Y: y_data})\n",
+    "    \n",
+    "    if step % 100 == 0:\n",
+    "        print('step:', step, 'w1:', sess.run(w1), 'w2:', sess.run(w2), 'w3:', sess.run(w3), 'b:', sess.run(b), 'cost:', sess.run(cost)) "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- Q1. 왜 바로 위의 코드처럼 선형회귀 때 같이 train만 세션을 실행한다면 오류가 생기는 것일까? \n",
+    "- Q2. 왜 x1과 x1_data를 구분하여 feed 시키는 형태로 식을 짠 것일까? "
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# (번외 1-2) Tensorflow를 활용한 Multi-Variable linear regression(Matrix 계산)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "tf.set_random_seed(12)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 12,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "x_data = [[73., 80., 75.],\n",
+    "          [93., 88., 93.],\n",
+    "          [89., 91., 90.],\n",
+    "          [96., 98., 100.],\n",
+    "          [73., 66., 70.]] # rank2, 5X3 matrix\n",
+    "y_data = [[152.],\n",
+    "          [185.],\n",
+    "          [180.],\n",
+    "          [196.],\n",
+    "          [142.]] # rank2, 5X1 matrix"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 13,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "X = tf.placeholder(tf.float32, shape = [None, 3]) # None : n개, 여기서는 5개\n",
+    "Y = tf.placeholder(tf.float32, shape = [None, 1])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 14,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "W = tf.Variable(tf.random_normal([3,1]), name = 'weight') # 3은 input의 개수, 1은 output의 개수\n",
+    "b = tf.Variable(tf.random_normal([1]), name = 'bias') # bias의 output개수는 Y의 output의 개수와 같아야 한다."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 15,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "hypothesis = tf.matmul(X,W) + b"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 16,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 18,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 W: [[ 0.90147442]\n",
+      " [ 0.22196172]\n",
+      " [ 1.0500567 ]] b: [ 1.31875741] cost: 777.853\n",
+      "step: 100 W: [[ 0.83768594]\n",
+      " [ 0.16684835]\n",
+      " [ 0.98683023]] b: [ 1.31817234] cost: 1.22004\n",
+      "step: 200 W: [[ 0.83450598]\n",
+      " [ 0.17204478]\n",
+      " [ 0.98484087]] b: [ 1.31829154] cost: 1.17892\n",
+      "step: 300 W: [[ 0.83142769]\n",
+      " [ 0.17711039]\n",
+      " [ 0.98287994]] b: [ 1.31841075] cost: 1.13992\n",
+      "step: 400 W: [[ 0.82844818]\n",
+      " [ 0.1820489 ]\n",
+      " [ 0.98094696]] b: [ 1.31852996] cost: 1.10289\n",
+      "step: 500 W: [[ 0.8255651 ]\n",
+      " [ 0.18686335]\n",
+      " [ 0.9790411 ]] b: [ 1.31864917] cost: 1.06772\n",
+      "step: 600 W: [[ 0.82277554]\n",
+      " [ 0.19155759]\n",
+      " [ 0.97716153]] b: [ 1.31876838] cost: 1.03434\n",
+      "step: 700 W: [[ 0.82007718]\n",
+      " [ 0.19613446]\n",
+      " [ 0.97530782]] b: [ 1.31888759] cost: 1.00264\n",
+      "step: 800 W: [[ 0.81746739]\n",
+      " [ 0.20059717]\n",
+      " [ 0.97347915]] b: [ 1.3190068] cost: 0.972558\n",
+      "step: 900 W: [[ 0.81494355]\n",
+      " [ 0.20494887]\n",
+      " [ 0.97167528]] b: [ 1.31912601] cost: 0.943961\n",
+      "step: 1000 W: [[ 0.81250346]\n",
+      " [ 0.20919232]\n",
+      " [ 0.96989542]] b: [ 1.31924522] cost: 0.916806\n",
+      "step: 1100 W: [[ 0.81014484]\n",
+      " [ 0.21333051]\n",
+      " [ 0.96813899]] b: [ 1.31936443] cost: 0.891003\n",
+      "step: 1200 W: [[ 0.80786568]\n",
+      " [ 0.21736608]\n",
+      " [ 0.96640539]] b: [ 1.31948364] cost: 0.866494\n",
+      "step: 1300 W: [[ 0.8056637 ]\n",
+      " [ 0.22130206]\n",
+      " [ 0.96469402]] b: [ 1.31960285] cost: 0.843198\n",
+      "step: 1400 W: [[ 0.80353671]\n",
+      " [ 0.22514062]\n",
+      " [ 0.96300441]] b: [ 1.31972206] cost: 0.821066\n",
+      "step: 1500 W: [[ 0.80148274]\n",
+      " [ 0.22888479]\n",
+      " [ 0.96133596]] b: [ 1.31984127] cost: 0.800021\n",
+      "step: 1600 W: [[ 0.79950005]\n",
+      " [ 0.23253685]\n",
+      " [ 0.95968795]] b: [ 1.31996047] cost: 0.780016\n",
+      "step: 1700 W: [[ 0.79758644]\n",
+      " [ 0.2360993 ]\n",
+      " [ 0.95806026]] b: [ 1.32007968] cost: 0.760986\n",
+      "step: 1800 W: [[ 0.79573995]\n",
+      " [ 0.23957449]\n",
+      " [ 0.95645237]] b: [ 1.32019889] cost: 0.742894\n",
+      "step: 1900 W: [[ 0.7939589 ]\n",
+      " [ 0.24296463]\n",
+      " [ 0.95486373]] b: [ 1.3203181] cost: 0.725681\n",
+      "step: 2000 W: [[ 0.79224133]\n",
+      " [ 0.24627224]\n",
+      " [ 0.9532938 ]] b: [ 1.32043731] cost: 0.709292\n",
+      "step: 2100 W: [[ 0.79058534]\n",
+      " [ 0.24949938]\n",
+      " [ 0.95174247]] b: [ 1.32055652] cost: 0.693715\n",
+      "step: 2200 W: [[ 0.78898978]\n",
+      " [ 0.25264817]\n",
+      " [ 0.95020884]] b: [ 1.32067573] cost: 0.678878\n",
+      "step: 2300 W: [[ 0.78745258]\n",
+      " [ 0.25572073]\n",
+      " [ 0.94869286]] b: [ 1.32079494] cost: 0.664756\n",
+      "step: 2400 W: [[ 0.7859723 ]\n",
+      " [ 0.258719  ]\n",
+      " [ 0.94719392]] b: [ 1.32091415] cost: 0.651304\n",
+      "step: 2500 W: [[ 0.78454745]\n",
+      " [ 0.2616449 ]\n",
+      " [ 0.94571161]] b: [ 1.32103336] cost: 0.638499\n",
+      "step: 2600 W: [[ 0.78317648]\n",
+      " [ 0.26450035]\n",
+      " [ 0.94424558]] b: [ 1.32115257] cost: 0.626298\n",
+      "step: 2700 W: [[ 0.78185761]\n",
+      " [ 0.26728749]\n",
+      " [ 0.94279563]] b: [ 1.32127178] cost: 0.61467\n",
+      "step: 2800 W: [[ 0.78058976]\n",
+      " [ 0.27000782]\n",
+      " [ 0.94136107]] b: [ 1.32139099] cost: 0.60359\n",
+      "step: 2900 W: [[ 0.77937138]\n",
+      " [ 0.27266335]\n",
+      " [ 0.93994182]] b: [ 1.3215102] cost: 0.593022\n",
+      "step: 3000 W: [[ 0.7782011 ]\n",
+      " [ 0.27525547]\n",
+      " [ 0.93853754]] b: [ 1.32162941] cost: 0.582928\n",
+      "step: 3100 W: [[ 0.77707762]\n",
+      " [ 0.27778602]\n",
+      " [ 0.93714762]] b: [ 1.32174861] cost: 0.573317\n",
+      "step: 3200 W: [[ 0.77599978]\n",
+      " [ 0.28025675]\n",
+      " [ 0.93577182]] b: [ 1.32186782] cost: 0.564134\n",
+      "step: 3300 W: [[ 0.77496618]\n",
+      " [ 0.28266913]\n",
+      " [ 0.93440992]] b: [ 1.32198703] cost: 0.555379\n",
+      "step: 3400 W: [[ 0.77397561]\n",
+      " [ 0.28502476]\n",
+      " [ 0.93306154]] b: [ 1.32210624] cost: 0.547003\n",
+      "step: 3500 W: [[ 0.773027  ]\n",
+      " [ 0.28732502]\n",
+      " [ 0.93172628]] b: [ 1.32222545] cost: 0.539025\n",
+      "step: 3600 W: [[ 0.77211887]\n",
+      " [ 0.28957152]\n",
+      " [ 0.93040407]] b: [ 1.32234466] cost: 0.531375\n",
+      "step: 3700 W: [[ 0.77125055]\n",
+      " [ 0.29176566]\n",
+      " [ 0.92909437]] b: [ 1.32246387] cost: 0.524079\n",
+      "step: 3800 W: [[ 0.77042073]\n",
+      " [ 0.2939086 ]\n",
+      " [ 0.92779702]] b: [ 1.32258308] cost: 0.517095\n",
+      "step: 3900 W: [[ 0.76962835]\n",
+      " [ 0.29600185]\n",
+      " [ 0.92651182]] b: [ 1.32270229] cost: 0.510421\n",
+      "step: 4000 W: [[ 0.76887214]\n",
+      " [ 0.29804686]\n",
+      " [ 0.92523855]] b: [ 1.3228215] cost: 0.50403\n",
+      "step: 4100 W: [[ 0.76815158]\n",
+      " [ 0.30004451]\n",
+      " [ 0.92397666]] b: [ 1.32294071] cost: 0.49791\n",
+      "step: 4200 W: [[ 0.76746511]\n",
+      " [ 0.30199641]\n",
+      " [ 0.92272633]] b: [ 1.32305992] cost: 0.492049\n",
+      "step: 4300 W: [[ 0.76681215]\n",
+      " [ 0.3039037 ]\n",
+      " [ 0.92148691]] b: [ 1.32317913] cost: 0.48643\n",
+      "step: 4400 W: [[ 0.7661916 ]\n",
+      " [ 0.30576771]\n",
+      " [ 0.92025816]] b: [ 1.32329834] cost: 0.481041\n",
+      "step: 4500 W: [[ 0.76560301]\n",
+      " [ 0.30758908]\n",
+      " [ 0.9190402 ]] b: [ 1.32341754] cost: 0.475876\n",
+      "step: 4600 W: [[ 0.76504481]\n",
+      " [ 0.30936936]\n",
+      " [ 0.91783243]] b: [ 1.32353675] cost: 0.470926\n",
+      "step: 4700 W: [[ 0.76451659]\n",
+      " [ 0.31110957]\n",
+      " [ 0.91663468]] b: [ 1.32365596] cost: 0.466159\n",
+      "step: 4800 W: [[ 0.76401734]\n",
+      " [ 0.31281081]\n",
+      " [ 0.91544682]] b: [ 1.32377517] cost: 0.461596\n",
+      "step: 4900 W: [[ 0.76354641]\n",
+      " [ 0.31447369]\n",
+      " [ 0.91426879]] b: [ 1.32389438] cost: 0.457208\n",
+      "step: 5000 W: [[ 0.76310301]\n",
+      " [ 0.31609967]\n",
+      " [ 0.91310024]] b: [ 1.32401359] cost: 0.45298\n",
+      "step: 5100 W: [[ 0.76268589]\n",
+      " [ 0.31768954]\n",
+      " [ 0.91194099]] b: [ 1.3241328] cost: 0.448921\n",
+      "step: 5200 W: [[ 0.76229471]\n",
+      " [ 0.31924438]\n",
+      " [ 0.91079092]] b: [ 1.32425201] cost: 0.445011\n",
+      "step: 5300 W: [[ 0.76192892]\n",
+      " [ 0.32076502]\n",
+      " [ 0.90964943]] b: [ 1.32437122] cost: 0.44124\n",
+      "step: 5400 W: [[ 0.76158762]\n",
+      " [ 0.32225221]\n",
+      " [ 0.90851682]] b: [ 1.32449043] cost: 0.437614\n",
+      "step: 5500 W: [[ 0.76127017]\n",
+      " [ 0.32370678]\n",
+      " [ 0.90739286]] b: [ 1.32460964] cost: 0.434117\n",
+      "step: 5600 W: [[ 0.76097584]\n",
+      " [ 0.32513002]\n",
+      " [ 0.90627706]] b: [ 1.32472885] cost: 0.430752\n",
+      "step: 5700 W: [[ 0.76070392]\n",
+      " [ 0.32652214]\n",
+      " [ 0.90516961]] b: [ 1.32484806] cost: 0.427491\n",
+      "step: 5800 W: [[ 0.76045364]\n",
+      " [ 0.32788461]\n",
+      " [ 0.90407014]] b: [ 1.32496727] cost: 0.424344\n",
+      "step: 5900 W: [[ 0.76022458]\n",
+      " [ 0.32921788]\n",
+      " [ 0.9029786 ]] b: [ 1.32508647] cost: 0.421309\n",
+      "step: 6000 W: [[ 0.76001608]\n",
+      " [ 0.33052266]\n",
+      " [ 0.90189475]] b: [ 1.32520568] cost: 0.418373\n",
+      "step: 6100 W: [[ 0.75982785]\n",
+      " [ 0.33179972]\n",
+      " [ 0.90081847]] b: [ 1.32532489] cost: 0.415535\n",
+      "step: 6200 W: [[ 0.75965899]\n",
+      " [ 0.33304992]\n",
+      " [ 0.8997494 ]] b: [ 1.3254441] cost: 0.412781\n",
+      "step: 6300 W: [[ 0.75950909]\n",
+      " [ 0.33427396]\n",
+      " [ 0.89868736]] b: [ 1.32556331] cost: 0.410118\n",
+      "step: 6400 W: [[ 0.75937748]\n",
+      " [ 0.33547217]\n",
+      " [ 0.89763284]] b: [ 1.32568252] cost: 0.407536\n",
+      "step: 6500 W: [[ 0.75926363]\n",
+      " [ 0.33664569]\n",
+      " [ 0.89658511]] b: [ 1.32580173] cost: 0.405033\n",
+      "step: 6600 W: [[ 0.75916702]\n",
+      " [ 0.33779496]\n",
+      " [ 0.89554441]] b: [ 1.32592094] cost: 0.402611\n",
+      "step: 6700 W: [[ 0.75908715]\n",
+      " [ 0.33892056]\n",
+      " [ 0.89451045]] b: [ 1.32604015] cost: 0.400246\n",
+      "step: 6800 W: [[ 0.75902343]\n",
+      " [ 0.34002322]\n",
+      " [ 0.89348316]] b: [ 1.32615936] cost: 0.397964\n",
+      "step: 6900 W: [[ 0.75897586]\n",
+      " [ 0.3411034 ]\n",
+      " [ 0.89246196]] b: [ 1.32627857] cost: 0.395735\n",
+      "step: 7000 W: [[ 0.7589438 ]\n",
+      " [ 0.34216186]\n",
+      " [ 0.89144713]] b: [ 1.32639778] cost: 0.393571\n",
+      "step: 7100 W: [[ 0.75892645]\n",
+      " [ 0.34319913]\n",
+      " [ 0.89043856]] b: [ 1.32651699] cost: 0.391456\n",
+      "step: 7200 W: [[ 0.75892401]\n",
+      " [ 0.34421557]\n",
+      " [ 0.88943601]] b: [ 1.3266362] cost: 0.389406\n",
+      "step: 7300 W: [[ 0.75893521]\n",
+      " [ 0.34521183]\n",
+      " [ 0.88843966]] b: [ 1.3267554] cost: 0.387415\n",
+      "step: 7400 W: [[ 0.75896001]\n",
+      " [ 0.34618849]\n",
+      " [ 0.8874492 ]] b: [ 1.32687461] cost: 0.38545\n",
+      "step: 7500 W: [[ 0.75899839]\n",
+      " [ 0.34714606]\n",
+      " [ 0.8864643 ]] b: [ 1.32699382] cost: 0.383553\n",
+      "step: 7600 W: [[ 0.75904959]\n",
+      " [ 0.34808478]\n",
+      " [ 0.88548529]] b: [ 1.32711303] cost: 0.381699\n",
+      "step: 7700 W: [[ 0.75911337]\n",
+      " [ 0.3490054 ]\n",
+      " [ 0.88451189]] b: [ 1.32723224] cost: 0.379891\n",
+      "step: 7800 W: [[ 0.75918919]\n",
+      " [ 0.34990829]\n",
+      " [ 0.88354391]] b: [ 1.32735145] cost: 0.378111\n",
+      "step: 7900 W: [[ 0.75927687]\n",
+      " [ 0.35079387]\n",
+      " [ 0.88258135]] b: [ 1.32747066] cost: 0.376376\n",
+      "step: 8000 W: [[ 0.75937605]\n",
+      " [ 0.35166267]\n",
+      " [ 0.88162392]] b: [ 1.32758987] cost: 0.374683\n",
+      "step: 8100 W: [[ 0.75948644]\n",
+      " [ 0.35251522]\n",
+      " [ 0.8806715 ]] b: [ 1.32770908] cost: 0.373025\n",
+      "step: 8200 W: [[ 0.75960767]\n",
+      " [ 0.35335174]\n",
+      " [ 0.87972432]] b: [ 1.32782829] cost: 0.371396\n",
+      "step: 8300 W: [[ 0.75973916]\n",
+      " [ 0.35417271]\n",
+      " [ 0.87878221]] b: [ 1.3279475] cost: 0.369802\n",
+      "step: 8400 W: [[ 0.75988048]\n",
+      " [ 0.35497862]\n",
+      " [ 0.87784529]] b: [ 1.32806671] cost: 0.368236\n",
+      "step: 8500 W: [[ 0.760032  ]\n",
+      " [ 0.35576969]\n",
+      " [ 0.87691289]] b: [ 1.32818592] cost: 0.366702\n",
+      "step: 8600 W: [[ 0.76019293]\n",
+      " [ 0.35654676]\n",
+      " [ 0.87598509]] b: [ 1.32830513] cost: 0.365198\n",
+      "step: 8700 W: [[ 0.76036328]\n",
+      " [ 0.35730967]\n",
+      " [ 0.87506193]] b: [ 1.32842433] cost: 0.363718\n",
+      "step: 8800 W: [[ 0.76054257]\n",
+      " [ 0.35805887]\n",
+      " [ 0.87414354]] b: [ 1.32854354] cost: 0.362279\n",
+      "step: 8900 W: [[ 0.76073003]\n",
+      " [ 0.35879472]\n",
+      " [ 0.87323016]] b: [ 1.32866275] cost: 0.360848\n",
+      "step: 9000 W: [[ 0.76092637]\n",
+      " [ 0.35951781]\n",
+      " [ 0.87232065]] b: [ 1.32878196] cost: 0.359446\n",
+      "step: 9100 W: [[ 0.76113117]\n",
+      " [ 0.36022824]\n",
+      " [ 0.87141526]] b: [ 1.32890117] cost: 0.358059\n",
+      "step: 9200 W: [[ 0.7613433 ]\n",
+      " [ 0.36092615]\n",
+      " [ 0.87051493]] b: [ 1.32902038] cost: 0.356705\n",
+      "step: 9300 W: [[ 0.76156348]\n",
+      " [ 0.36161214]\n",
+      " [ 0.86961865]] b: [ 1.32913959] cost: 0.355369\n",
+      "step: 9400 W: [[ 0.76179099]\n",
+      " [ 0.36228645]\n",
+      " [ 0.86872643]] b: [ 1.3292588] cost: 0.354053\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 9500 W: [[ 0.76202601]\n",
+      " [ 0.36294928]\n",
+      " [ 0.86783814]] b: [ 1.32937801] cost: 0.352758\n",
+      "step: 9600 W: [[ 0.76226735]\n",
+      " [ 0.36360085]\n",
+      " [ 0.86695474]] b: [ 1.32949722] cost: 0.351478\n",
+      "step: 9700 W: [[ 0.76251602]\n",
+      " [ 0.36424202]\n",
+      " [ 0.86607438]] b: [ 1.32961643] cost: 0.350212\n",
+      "step: 9800 W: [[ 0.76277083]\n",
+      " [ 0.36487257]\n",
+      " [ 0.86519855]] b: [ 1.32973564] cost: 0.348977\n",
+      "step: 9900 W: [[ 0.76303208]\n",
+      " [ 0.36549261]\n",
+      " [ 0.8643266 ]] b: [ 1.32985485] cost: 0.347746\n",
+      "step: 10000 W: [[ 0.76329964]\n",
+      " [ 0.36610276]\n",
+      " [ 0.86345816]] b: [ 1.32997406] cost: 0.346531\n",
+      "step: 10100 W: [[ 0.76357305]\n",
+      " [ 0.366703  ]\n",
+      " [ 0.86259371]] b: [ 1.33009326] cost: 0.345336\n",
+      "step: 10200 W: [[ 0.76385248]\n",
+      " [ 0.36729369]\n",
+      " [ 0.8617329 ]] b: [ 1.33021247] cost: 0.344156\n",
+      "step: 10300 W: [[ 0.76413739]\n",
+      " [ 0.36787537]\n",
+      " [ 0.86087543]] b: [ 1.33033168] cost: 0.342989\n",
+      "step: 10400 W: [[ 0.76442742]\n",
+      " [ 0.36844766]\n",
+      " [ 0.86002225]] b: [ 1.33045089] cost: 0.341834\n",
+      "step: 10500 W: [[ 0.764723  ]\n",
+      " [ 0.36901146]\n",
+      " [ 0.85917193]] b: [ 1.3305701] cost: 0.340689\n",
+      "step: 10600 W: [[ 0.76502347]\n",
+      " [ 0.3695665 ]\n",
+      " [ 0.85832542]] b: [ 1.33068931] cost: 0.339558\n",
+      "step: 10700 W: [[ 0.76532817]\n",
+      " [ 0.37011328]\n",
+      " [ 0.85748291]] b: [ 1.33080852] cost: 0.338442\n",
+      "step: 10800 W: [[ 0.76563829]\n",
+      " [ 0.3706519 ]\n",
+      " [ 0.85664308]] b: [ 1.33092773] cost: 0.337336\n",
+      "step: 10900 W: [[ 0.76595294]\n",
+      " [ 0.37118208]\n",
+      " [ 0.85580713]] b: [ 1.33104694] cost: 0.336255\n",
+      "step: 11000 W: [[ 0.76627254]\n",
+      " [ 0.37170491]\n",
+      " [ 0.85497361]] b: [ 1.33116615] cost: 0.335173\n",
+      "step: 11100 W: [[ 0.76659548]\n",
+      " [ 0.37221998]\n",
+      " [ 0.85414433]] b: [ 1.33128536] cost: 0.334091\n",
+      "step: 11200 W: [[ 0.76692289]\n",
+      " [ 0.3727279 ]\n",
+      " [ 0.85331762]] b: [ 1.33140457] cost: 0.333029\n",
+      "step: 11300 W: [[ 0.76725411]\n",
+      " [ 0.37322819]\n",
+      " [ 0.85249478]] b: [ 1.33152378] cost: 0.331976\n",
+      "step: 11400 W: [[ 0.76758856]\n",
+      " [ 0.37372196]\n",
+      " [ 0.85167533]] b: [ 1.33163631] cost: 0.330936\n",
+      "step: 11500 W: [[ 0.76792759]\n",
+      " [ 0.37420839]\n",
+      " [ 0.85085857]] b: [ 1.3317436] cost: 0.329905\n",
+      "step: 11600 W: [[ 0.76827013]\n",
+      " [ 0.37468842]\n",
+      " [ 0.85004485]] b: [ 1.33185089] cost: 0.328886\n",
+      "step: 11700 W: [[ 0.76861596]\n",
+      " [ 0.37516177]\n",
+      " [ 0.84923416]] b: [ 1.33195817] cost: 0.327873\n",
+      "step: 11800 W: [[ 0.76896572]\n",
+      " [ 0.3756285 ]\n",
+      " [ 0.8484264 ]] b: [ 1.33206546] cost: 0.326868\n",
+      "step: 11900 W: [[ 0.76931798]\n",
+      " [ 0.37608942]\n",
+      " [ 0.84762168]] b: [ 1.33217275] cost: 0.325872\n",
+      "step: 12000 W: [[ 0.7696743 ]\n",
+      " [ 0.37654385]\n",
+      " [ 0.84681958]] b: [ 1.33228004] cost: 0.324881\n",
+      "step: 12100 W: [[ 0.77003253]\n",
+      " [ 0.37699288]\n",
+      " [ 0.84602076]] b: [ 1.33238733] cost: 0.323909\n",
+      "step: 12200 W: [[ 0.77039486]\n",
+      " [ 0.37743577]\n",
+      " [ 0.84522408]] b: [ 1.33249462] cost: 0.322927\n",
+      "step: 12300 W: [[ 0.77075875]\n",
+      " [ 0.37787312]\n",
+      " [ 0.84443116]] b: [ 1.3326019] cost: 0.321972\n",
+      "step: 12400 W: [[ 0.77112585]\n",
+      " [ 0.37830499]\n",
+      " [ 0.84364069]] b: [ 1.33270919] cost: 0.321013\n",
+      "step: 12500 W: [[ 0.77149516]\n",
+      " [ 0.37873113]\n",
+      " [ 0.84285343]] b: [ 1.33281648] cost: 0.320059\n",
+      "step: 12600 W: [[ 0.7718668 ]\n",
+      " [ 0.3791528 ]\n",
+      " [ 0.84206849]] b: [ 1.33292377] cost: 0.319111\n",
+      "step: 12700 W: [[ 0.77224123]\n",
+      " [ 0.37956834]\n",
+      " [ 0.84128678]] b: [ 1.33303106] cost: 0.318176\n",
+      "step: 12800 W: [[ 0.77261722]\n",
+      " [ 0.37997997]\n",
+      " [ 0.84050745]] b: [ 1.33313835] cost: 0.317252\n",
+      "step: 12900 W: [[ 0.77299565]\n",
+      " [ 0.38038582]\n",
+      " [ 0.83973122]] b: [ 1.33324564] cost: 0.316324\n",
+      "step: 13000 W: [[ 0.77337676]\n",
+      " [ 0.38078722]\n",
+      " [ 0.83895689]] b: [ 1.33335292] cost: 0.315408\n",
+      "step: 13100 W: [[ 0.77375853]\n",
+      " [ 0.38118413]\n",
+      " [ 0.83818626]] b: [ 1.33346021] cost: 0.3145\n",
+      "step: 13200 W: [[ 0.7741434 ]\n",
+      " [ 0.38157624]\n",
+      " [ 0.83741736]] b: [ 1.3335675] cost: 0.313594\n",
+      "step: 13300 W: [[ 0.77453053]\n",
+      " [ 0.381964  ]\n",
+      " [ 0.83665055]] b: [ 1.33367479] cost: 0.312696\n",
+      "step: 13400 W: [[ 0.77491814]\n",
+      " [ 0.38234764]\n",
+      " [ 0.83588725]] b: [ 1.33378208] cost: 0.311805\n",
+      "step: 13500 W: [[ 0.77530831]\n",
+      " [ 0.38272718]\n",
+      " [ 0.83512557]] b: [ 1.33388937] cost: 0.310914\n",
+      "step: 13600 W: [[ 0.77570015]\n",
+      " [ 0.38310167]\n",
+      " [ 0.83436722]] b: [ 1.33399665] cost: 0.310028\n",
+      "step: 13700 W: [[ 0.77609336]\n",
+      " [ 0.38347292]\n",
+      " [ 0.83361065]] b: [ 1.33410394] cost: 0.309155\n",
+      "step: 13800 W: [[ 0.77648687]\n",
+      " [ 0.38384035]\n",
+      " [ 0.83285755]] b: [ 1.33421123] cost: 0.308283\n",
+      "step: 13900 W: [[ 0.77688247]\n",
+      " [ 0.38420376]\n",
+      " [ 0.83210641]] b: [ 1.33431852] cost: 0.307429\n",
+      "step: 14000 W: [[ 0.77728075]\n",
+      " [ 0.38456333]\n",
+      " [ 0.83135647]] b: [ 1.33442581] cost: 0.306558\n",
+      "step: 14100 W: [[ 0.77767962]\n",
+      " [ 0.3849186 ]\n",
+      " [ 0.8306101 ]] b: [ 1.3345331] cost: 0.305711\n",
+      "step: 14200 W: [[ 0.77807915]\n",
+      " [ 0.38527131]\n",
+      " [ 0.82986552]] b: [ 1.33464038] cost: 0.304854\n",
+      "step: 14300 W: [[ 0.77847862]\n",
+      " [ 0.38562033]\n",
+      " [ 0.82912481]] b: [ 1.33474767] cost: 0.304016\n",
+      "step: 14400 W: [[ 0.77888024]\n",
+      " [ 0.38596568]\n",
+      " [ 0.82838553]] b: [ 1.33485496] cost: 0.303173\n",
+      "step: 14500 W: [[ 0.7792843 ]\n",
+      " [ 0.38630769]\n",
+      " [ 0.82764721]] b: [ 1.33496225] cost: 0.302333\n",
+      "step: 14600 W: [[ 0.77968872]\n",
+      " [ 0.38664573]\n",
+      " [ 0.8269124 ]] b: [ 1.33506954] cost: 0.301508\n",
+      "step: 14700 W: [[ 0.78009373]\n",
+      " [ 0.3869814 ]\n",
+      " [ 0.82617927]] b: [ 1.33517683] cost: 0.300687\n",
+      "step: 14800 W: [[ 0.78049904]\n",
+      " [ 0.3873139 ]\n",
+      " [ 0.82544899]] b: [ 1.33528411] cost: 0.299866\n",
+      "step: 14900 W: [[ 0.78090459]\n",
+      " [ 0.38764343]\n",
+      " [ 0.82472146]] b: [ 1.3353914] cost: 0.299047\n",
+      "step: 15000 W: [[ 0.78131157]\n",
+      " [ 0.38797027]\n",
+      " [ 0.82399517]] b: [ 1.33549869] cost: 0.298242\n",
+      "step: 15100 W: [[ 0.78171927]\n",
+      " [ 0.3882935 ]\n",
+      " [ 0.82327181]] b: [ 1.33560598] cost: 0.297437\n",
+      "step: 15200 W: [[ 0.78212792]\n",
+      " [ 0.38861379]\n",
+      " [ 0.8225503 ]] b: [ 1.33571327] cost: 0.296632\n",
+      "step: 15300 W: [[ 0.78253824]\n",
+      " [ 0.38893136]\n",
+      " [ 0.82182986]] b: [ 1.33582056] cost: 0.295833\n",
+      "step: 15400 W: [[ 0.78294879]\n",
+      " [ 0.3892456 ]\n",
+      " [ 0.82111257]] b: [ 1.33592784] cost: 0.295038\n",
+      "step: 15500 W: [[ 0.78335959]\n",
+      " [ 0.38955757]\n",
+      " [ 0.82039708]] b: [ 1.33603513] cost: 0.294252\n",
+      "step: 15600 W: [[ 0.78377074]\n",
+      " [ 0.38986737]\n",
+      " [ 0.81968355]] b: [ 1.33614242] cost: 0.293465\n",
+      "step: 15700 W: [[ 0.78418177]\n",
+      " [ 0.39017409]\n",
+      " [ 0.81897312]] b: [ 1.33624971] cost: 0.292686\n",
+      "step: 15800 W: [[ 0.78459311]\n",
+      " [ 0.39047909]\n",
+      " [ 0.81826401]] b: [ 1.336357] cost: 0.291912\n",
+      "step: 15900 W: [[ 0.78500444]\n",
+      " [ 0.39078146]\n",
+      " [ 0.81755763]] b: [ 1.33646429] cost: 0.29114\n",
+      "step: 16000 W: [[ 0.78541595]\n",
+      " [ 0.39108139]\n",
+      " [ 0.81685346]] b: [ 1.33657157] cost: 0.290375\n",
+      "step: 16100 W: [[ 0.78582829]\n",
+      " [ 0.39137906]\n",
+      " [ 0.81615055]] b: [ 1.33667886] cost: 0.289605\n",
+      "step: 16200 W: [[ 0.78624034]\n",
+      " [ 0.39167431]\n",
+      " [ 0.81545049]] b: [ 1.33678615] cost: 0.28885\n",
+      "step: 16300 W: [[ 0.78665245]\n",
+      " [ 0.39196709]\n",
+      " [ 0.8147527 ]] b: [ 1.33689344] cost: 0.288099\n",
+      "step: 16400 W: [[ 0.78706563]\n",
+      " [ 0.39225811]\n",
+      " [ 0.81405562]] b: [ 1.33700073] cost: 0.287349\n",
+      "step: 16500 W: [[ 0.7874791 ]\n",
+      " [ 0.39254656]\n",
+      " [ 0.81336081]] b: [ 1.33710802] cost: 0.286592\n",
+      "step: 16600 W: [[ 0.78789228]\n",
+      " [ 0.39283264]\n",
+      " [ 0.81266868]] b: [ 1.3372153] cost: 0.285852\n",
+      "step: 16700 W: [[ 0.78830606]\n",
+      " [ 0.39311704]\n",
+      " [ 0.81197751]] b: [ 1.33732259] cost: 0.285109\n",
+      "step: 16800 W: [[ 0.78871989]\n",
+      " [ 0.39339918]\n",
+      " [ 0.81128865]] b: [ 1.33742988] cost: 0.284372\n",
+      "step: 16900 W: [[ 0.78913295]\n",
+      " [ 0.39367902]\n",
+      " [ 0.81060272]] b: [ 1.33753717] cost: 0.283642\n",
+      "step: 17000 W: [[ 0.78954709]\n",
+      " [ 0.39395729]\n",
+      " [ 0.80991727]] b: [ 1.33764446] cost: 0.282916\n",
+      "step: 17100 W: [[ 0.7899611 ]\n",
+      " [ 0.39423376]\n",
+      " [ 0.8092339 ]] b: [ 1.33775175] cost: 0.282184\n",
+      "step: 17200 W: [[ 0.79037416]\n",
+      " [ 0.39450783]\n",
+      " [ 0.80855364]] b: [ 1.33785903] cost: 0.281468\n",
+      "step: 17300 W: [[ 0.79078782]\n",
+      " [ 0.39478058]\n",
+      " [ 0.80787414]] b: [ 1.33796632] cost: 0.280753\n",
+      "step: 17400 W: [[ 0.79120183]\n",
+      " [ 0.3950513 ]\n",
+      " [ 0.80719638]] b: [ 1.3380729] cost: 0.280031\n",
+      "step: 17500 W: [[ 0.79161465]\n",
+      " [ 0.39532012]\n",
+      " [ 0.80652177]] b: [ 1.33816838] cost: 0.279326\n",
+      "step: 17600 W: [[ 0.79202777]\n",
+      " [ 0.39558762]\n",
+      " [ 0.80584818]] b: [ 1.33826375] cost: 0.278624\n",
+      "step: 17700 W: [[ 0.79244089]\n",
+      " [ 0.39585343]\n",
+      " [ 0.80517608]] b: [ 1.33835912] cost: 0.277917\n",
+      "step: 17800 W: [[ 0.7928527 ]\n",
+      " [ 0.39611733]\n",
+      " [ 0.80450732]] b: [ 1.33845448] cost: 0.277224\n",
+      "step: 17900 W: [[ 0.79326493]\n",
+      " [ 0.39637962]\n",
+      " [ 0.80383968]] b: [ 1.33854985] cost: 0.27653\n",
+      "step: 18000 W: [[ 0.79367757]\n",
+      " [ 0.39664063]\n",
+      " [ 0.80317301]] b: [ 1.33864522] cost: 0.275837\n",
+      "step: 18100 W: [[ 0.79408896]\n",
+      " [ 0.39689973]\n",
+      " [ 0.80250937]] b: [ 1.33874059] cost: 0.27515\n",
+      "step: 18200 W: [[ 0.79450023]\n",
+      " [ 0.39715713]\n",
+      " [ 0.80184752]] b: [ 1.33883595] cost: 0.274465\n",
+      "step: 18300 W: [[ 0.7949118 ]\n",
+      " [ 0.39741355]\n",
+      " [ 0.80118638]] b: [ 1.33893132] cost: 0.273782\n",
+      "step: 18400 W: [[ 0.79532307]\n",
+      " [ 0.39766794]\n",
+      " [ 0.80052763]] b: [ 1.33902669] cost: 0.273102\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 18500 W: [[ 0.79573423]\n",
+      " [ 0.39792034]\n",
+      " [ 0.79987085]] b: [ 1.33912206] cost: 0.272429\n",
+      "step: 18600 W: [[ 0.79614544]\n",
+      " [ 0.39817151]\n",
+      " [ 0.79921514]] b: [ 1.33921742] cost: 0.271761\n",
+      "step: 18700 W: [[ 0.79655647]\n",
+      " [ 0.39842191]\n",
+      " [ 0.79856056]] b: [ 1.33931279] cost: 0.271091\n",
+      "step: 18800 W: [[ 0.79696739]\n",
+      " [ 0.3986699 ]\n",
+      " [ 0.79790837]] b: [ 1.33940816] cost: 0.270424\n",
+      "step: 18900 W: [[ 0.79737759]\n",
+      " [ 0.3989166 ]\n",
+      " [ 0.79725808]] b: [ 1.33950353] cost: 0.269757\n",
+      "step: 19000 W: [[ 0.79778832]\n",
+      " [ 0.39916232]\n",
+      " [ 0.79660839]] b: [ 1.33959889] cost: 0.269101\n",
+      "step: 19100 W: [[ 0.79819816]\n",
+      " [ 0.39940673]\n",
+      " [ 0.79596084]] b: [ 1.33969426] cost: 0.268446\n",
+      "step: 19200 W: [[ 0.7986064 ]\n",
+      " [ 0.39964977]\n",
+      " [ 0.79531622]] b: [ 1.33978963] cost: 0.2678\n",
+      "step: 19300 W: [[ 0.79901481]\n",
+      " [ 0.39989159]\n",
+      " [ 0.79467243]] b: [ 1.339885] cost: 0.267148\n",
+      "step: 19400 W: [[ 0.79942274]\n",
+      " [ 0.40013275]\n",
+      " [ 0.79403001]] b: [ 1.33998036] cost: 0.2665\n",
+      "step: 19500 W: [[ 0.79982877]\n",
+      " [ 0.40037233]\n",
+      " [ 0.79339093]] b: [ 1.34007573] cost: 0.265863\n",
+      "step: 19600 W: [[ 0.80023474]\n",
+      " [ 0.4006108 ]\n",
+      " [ 0.79275304]] b: [ 1.3401711] cost: 0.265223\n",
+      "step: 19700 W: [[ 0.80064064]\n",
+      " [ 0.40084857]\n",
+      " [ 0.79211587]] b: [ 1.34026647] cost: 0.264583\n",
+      "step: 19800 W: [[ 0.80104589]\n",
+      " [ 0.40108454]\n",
+      " [ 0.7914812 ]] b: [ 1.34036183] cost: 0.263954\n",
+      "step: 19900 W: [[ 0.80145109]\n",
+      " [ 0.40131888]\n",
+      " [ 0.79084826]] b: [ 1.3404572] cost: 0.263324\n",
+      "step: 20000 W: [[ 0.80185616]\n",
+      " [ 0.40155229]\n",
+      " [ 0.79021621]] b: [ 1.34055257] cost: 0.262702\n",
+      "8.924750804901123\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    t1 = time.time()\n",
+    "    \n",
+    "    for step in range(20001):\n",
+    "        cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict = {X : x_data, Y : y_data})\n",
+    "        if step % 100 == 0:\n",
+    "            print('step:', step, 'W:', sess.run(W), 'b:', sess.run(b), 'cost:', cost_val)\n",
+    "            \n",
+    "    t2 = time.time()\n",
+    "    print(t2-t1)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Matrix계산 시간이 조금 더 적게 걸렸다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# (번외 1-3) Tensorflow를 활용한 Logistic Regression"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 19,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "tf.set_random_seed(13)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 20,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "x_data = [[1, 2],\n",
+    "          [2, 3],\n",
+    "          [3, 1],\n",
+    "          [4, 3],\n",
+    "          [5, 3],\n",
+    "          [6, 2]]\n",
+    "y_data = [[0],\n",
+    "          [0],\n",
+    "          [0],\n",
+    "          [1],\n",
+    "          [1],\n",
+    "          [1]] # 0 or 1 binary data"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 21,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "X = tf.placeholder(tf.float32, shape = [None, 2])\n",
+    "Y = tf.placeholder(tf.float32, shape = [None, 1])"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 22,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "W = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
+    "b = tf.Variable(tf.random_normal([1]), name = 'bias')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 23,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
+    "# tf.div(1., 1. + tf.exp(tf.matmul(X,W) + b))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 24,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 25,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 26,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) # hypotheis > 0.5이면 T, < 0.5이면 F이고 이를 float32 dtype으로 바꾸면 1, 0이 된다.\n",
+    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32)) # predicted == Y 이면 T, 아니면 F이고 이를 1, 0으로 바꾼다."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 27,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 W: [[-0.02034846]\n",
+      " [-1.31082821]] b: [-1.15761578] 2.47863\n",
+      "step: 100 W: [[ 0.89645308]\n",
+      " [-0.76321447]] b: [-1.02054465] 0.418692\n",
+      "step: 200 W: [[ 0.92906427]\n",
+      " [-0.71724498]] b: [-1.09232831] 0.409346\n",
+      "step: 300 W: [[ 0.93433213]\n",
+      " [-0.69088382]] b: [-1.17067838] 0.402482\n",
+      "step: 400 W: [[ 0.93860853]\n",
+      " [-0.66614878]] b: [-1.24851048] 0.395792\n",
+      "step: 500 W: [[ 0.94310093]\n",
+      " [-0.64215589]] b: [-1.32548892] 0.389269\n",
+      "step: 600 W: [[ 0.94784391]\n",
+      " [-0.61882919]] b: [-1.40159774] 0.382909\n",
+      "step: 700 W: [[ 0.95281637]\n",
+      " [-0.59613454]] b: [-1.47683775] 0.376707\n",
+      "step: 800 W: [[ 0.95799607]\n",
+      " [-0.57404137]] b: [-1.55121231] 0.370659\n",
+      "step: 900 W: [[ 0.96336418]\n",
+      " [-0.55252415]] b: [-1.62472534] 0.364762\n",
+      "step: 1000 W: [[ 0.96890223]\n",
+      " [-0.53155613]] b: [-1.69738197] 0.359011\n",
+      "step: 1100 W: [[ 0.97459376]\n",
+      " [-0.5111137 ]] b: [-1.76918864] 0.353404\n",
+      "step: 1200 W: [[ 0.98042375]\n",
+      " [-0.49117476]] b: [-1.84015226] 0.347935\n",
+      "step: 1300 W: [[ 0.98637837]\n",
+      " [-0.47171846]] b: [-1.91028023] 0.342602\n",
+      "step: 1400 W: [[ 0.99244547]\n",
+      " [-0.45272562]] b: [-1.97958112] 0.337401\n",
+      "step: 1500 W: [[ 0.9986133 ]\n",
+      " [-0.43417799]] b: [-2.04806376] 0.332328\n",
+      "step: 1600 W: [[ 1.00487137]\n",
+      " [-0.41605863]] b: [-2.11573625] 0.32738\n",
+      "step: 1700 W: [[ 1.01121056]\n",
+      " [-0.3983517 ]] b: [-2.1826086] 0.322553\n",
+      "step: 1800 W: [[ 1.0176214 ]\n",
+      " [-0.38104177]] b: [-2.24869156] 0.317845\n",
+      "step: 1900 W: [[ 1.02409649]\n",
+      " [-0.36411461]] b: [-2.31399584] 0.313251\n",
+      "step: 2000 W: [[ 1.03062832]\n",
+      " [-0.34755677]] b: [-2.3785305] 0.308768\n",
+      "step: 2100 W: [[ 1.03720975]\n",
+      " [-0.33135527]] b: [-2.44230676] 0.304394\n",
+      "step: 2200 W: [[ 1.04383433]\n",
+      " [-0.31549826]] b: [-2.50533509] 0.300126\n",
+      "step: 2300 W: [[ 1.05049706]\n",
+      " [-0.29997391]] b: [-2.56762695] 0.295959\n",
+      "step: 2400 W: [[ 1.05719244]\n",
+      " [-0.28477132]] b: [-2.62919259] 0.291892\n",
+      "step: 2500 W: [[ 1.06391525]\n",
+      " [-0.26988003]] b: [-2.69004345] 0.287921\n",
+      "step: 2600 W: [[ 1.07066083]\n",
+      " [-0.25529003]] b: [-2.75019026] 0.284045\n",
+      "step: 2700 W: [[ 1.07742572]\n",
+      " [-0.24099201]] b: [-2.80964375] 0.280259\n",
+      "step: 2800 W: [[ 1.08420515]\n",
+      " [-0.22697665]] b: [-2.86841536] 0.276562\n",
+      "step: 2900 W: [[ 1.09099698]\n",
+      " [-0.21323514]] b: [-2.92651582] 0.272951\n",
+      "step: 3000 W: [[ 1.09779489]\n",
+      " [-0.19975851]] b: [-2.98395562] 0.269423\n",
+      "step: 3100 W: [[ 1.10460126]\n",
+      " [-0.18654053]] b: [-3.04074645] 0.265976\n",
+      "step: 3200 W: [[ 1.11140811]\n",
+      " [-0.17357241]] b: [-3.09689879] 0.262608\n",
+      "step: 3300 W: [[ 1.11821496]\n",
+      " [-0.1608461 ]] b: [-3.15242243] 0.259316\n",
+      "step: 3400 W: [[ 1.12501585]\n",
+      " [-0.14835405]] b: [-3.20732808] 0.256098\n",
+      "step: 3500 W: [[ 1.13181114]\n",
+      " [-0.13608816]] b: [-3.26162481] 0.252953\n",
+      "step: 3600 W: [[ 1.13860118]\n",
+      " [-0.124045  ]] b: [-3.31532502] 0.249878\n",
+      "step: 3700 W: [[ 1.145383  ]\n",
+      " [-0.11221681]] b: [-3.36843824] 0.24687\n",
+      "step: 3800 W: [[ 1.15215337]\n",
+      " [-0.10059813]] b: [-3.42097425] 0.243929\n",
+      "step: 3900 W: [[ 1.15891087]\n",
+      " [-0.08918301]] b: [-3.47294259] 0.241052\n",
+      "step: 4000 W: [[ 1.16565454]\n",
+      " [-0.07796564]] b: [-3.52435374] 0.238237\n",
+      "step: 4100 W: [[ 1.17238355]\n",
+      " [-0.06694048]] b: [-3.57521677] 0.235482\n",
+      "step: 4200 W: [[ 1.17909598]\n",
+      " [-0.05610241]] b: [-3.62554073] 0.232787\n",
+      "step: 4300 W: [[ 1.18579006]\n",
+      " [-0.04544615]] b: [-3.67533517] 0.230149\n",
+      "step: 4400 W: [[ 1.19246542]\n",
+      " [-0.0349669 ]] b: [-3.7246089] 0.227566\n",
+      "step: 4500 W: [[ 1.19912159]\n",
+      " [-0.0246597 ]] b: [-3.77337146] 0.225037\n",
+      "step: 4600 W: [[ 1.20575595]\n",
+      " [-0.01452009]] b: [-3.82163072] 0.222561\n",
+      "step: 4700 W: [[ 1.21236861]\n",
+      " [-0.00454365]] b: [-3.86939597] 0.220136\n",
+      "step: 4800 W: [[ 1.21895897]\n",
+      " [ 0.00527395]] b: [-3.91667533] 0.21776\n",
+      "step: 4900 W: [[ 1.22552598]\n",
+      " [ 0.01493672]] b: [-3.9634769] 0.215433\n",
+      "step: 5000 W: [[ 1.23206925]\n",
+      " [ 0.02444886]] b: [-4.00980949] 0.213152\n",
+      "step: 5100 W: [[ 1.23858821]\n",
+      " [ 0.03381412]] b: [-4.05568027] 0.210918\n",
+      "step: 5200 W: [[ 1.24508202]\n",
+      " [ 0.04303619]] b: [-4.10109711] 0.208727\n",
+      "step: 5300 W: [[ 1.25155044]\n",
+      " [ 0.0521188 ]] b: [-4.14606905] 0.20658\n",
+      "step: 5400 W: [[ 1.25799239]\n",
+      " [ 0.06106546]] b: [-4.19060326] 0.204475\n",
+      "step: 5500 W: [[ 1.26440787]\n",
+      " [ 0.06987963]] b: [-4.23470545] 0.202411\n",
+      "step: 5600 W: [[ 1.2707969 ]\n",
+      " [ 0.07856444]] b: [-4.27838373] 0.200387\n",
+      "step: 5700 W: [[ 1.27715921]\n",
+      " [ 0.08712303]] b: [-4.32164478] 0.198401\n",
+      "step: 5800 W: [[ 1.28349316]\n",
+      " [ 0.09555851]] b: [-4.36449671] 0.196453\n",
+      "step: 5900 W: [[ 1.28979969]\n",
+      " [ 0.10387391]] b: [-4.40694427] 0.194542\n",
+      "step: 6000 W: [[ 1.29607892]\n",
+      " [ 0.11207202]] b: [-4.44899654] 0.192667\n",
+      "step: 6100 W: [[ 1.30232918]\n",
+      " [ 0.12015576]] b: [-4.49065924] 0.190827\n",
+      "step: 6200 W: [[ 1.30855179]\n",
+      " [ 0.12812771]] b: [-4.53193808] 0.18902\n",
+      "step: 6300 W: [[ 1.31474614]\n",
+      " [ 0.13599052]] b: [-4.57283974] 0.187247\n",
+      "step: 6400 W: [[ 1.32091117]\n",
+      " [ 0.14374678]] b: [-4.61336994] 0.185505\n",
+      "step: 6500 W: [[ 1.32704854]\n",
+      " [ 0.1513989 ]] b: [-4.65353632] 0.183796\n",
+      "step: 6600 W: [[ 1.33315635]\n",
+      " [ 0.15894929]] b: [-4.69334221] 0.182117\n",
+      "step: 6700 W: [[ 1.33923578]\n",
+      " [ 0.16640039]] b: [-4.73279619] 0.180467\n",
+      "step: 6800 W: [[ 1.34528661]\n",
+      " [ 0.17375436]] b: [-4.77190256] 0.178847\n",
+      "step: 6900 W: [[ 1.35130799]\n",
+      " [ 0.18101339]] b: [-4.81066561] 0.177255\n",
+      "step: 7000 W: [[ 1.35730159]\n",
+      " [ 0.18817955]] b: [-4.84909296] 0.175691\n",
+      "step: 7100 W: [[ 1.3632654 ]\n",
+      " [ 0.19525485]] b: [-4.88718939] 0.174154\n",
+      "step: 7200 W: [[ 1.36920094]\n",
+      " [ 0.20224135]] b: [-4.92495823] 0.172643\n",
+      "step: 7300 W: [[ 1.37510765]\n",
+      " [ 0.20914102]] b: [-4.96240711] 0.171158\n",
+      "step: 7400 W: [[ 1.3809855 ]\n",
+      " [ 0.21595572]] b: [-4.99954033] 0.169698\n",
+      "step: 7500 W: [[ 1.38683534]\n",
+      " [ 0.22268721]] b: [-5.03636122] 0.168263\n",
+      "step: 7600 W: [[ 1.39265561]\n",
+      " [ 0.22933728]] b: [-5.07287693] 0.166851\n",
+      "step: 7700 W: [[ 1.39844799]\n",
+      " [ 0.23590772]] b: [-5.10908985] 0.165462\n",
+      "step: 7800 W: [[ 1.40421188]\n",
+      " [ 0.24240009]] b: [-5.14500618] 0.164097\n",
+      "step: 7900 W: [[ 1.40994716]\n",
+      " [ 0.24881622]] b: [-5.18063021] 0.162754\n",
+      "step: 8000 W: [[ 1.4156549 ]\n",
+      " [ 0.25515759]] b: [-5.21596622] 0.161432\n",
+      "step: 8100 W: [[ 1.42133367]\n",
+      " [ 0.26142564]] b: [-5.25101805] 0.160132\n",
+      "step: 8200 W: [[ 1.42698479]\n",
+      " [ 0.26762199]] b: [-5.28579092] 0.158852\n",
+      "step: 8300 W: [[ 1.4326086 ]\n",
+      " [ 0.27374807]] b: [-5.32028866] 0.157593\n",
+      "step: 8400 W: [[ 1.43820393]\n",
+      " [ 0.27980527]] b: [-5.35451412] 0.156353\n",
+      "step: 8500 W: [[ 1.44377184]\n",
+      " [ 0.28579509]] b: [-5.38847351] 0.155133\n",
+      "step: 8600 W: [[ 1.44931281]\n",
+      " [ 0.29171857]] b: [-5.42216825] 0.153932\n",
+      "step: 8700 W: [[ 1.454826  ]\n",
+      " [ 0.29757723]] b: [-5.45560551] 0.152749\n",
+      "step: 8800 W: [[ 1.46031189]\n",
+      " [ 0.30337241]] b: [-5.48878479] 0.151584\n",
+      "step: 8900 W: [[ 1.4657712 ]\n",
+      " [ 0.30910519]] b: [-5.52171278] 0.150437\n",
+      "step: 9000 W: [[ 1.4712038 ]\n",
+      " [ 0.31477678]] b: [-5.55439281] 0.149307\n",
+      "step: 9100 W: [[ 1.47660935]\n",
+      " [ 0.32038832]] b: [-5.58682823] 0.148194\n",
+      "step: 9200 W: [[ 1.48198819]\n",
+      " [ 0.32594115]] b: [-5.61902189] 0.147098\n",
+      "step: 9300 W: [[ 1.48734093]\n",
+      " [ 0.33143616]] b: [-5.65097713] 0.146018\n",
+      "step: 9400 W: [[ 1.49266756]\n",
+      " [ 0.33687457]] b: [-5.6826973] 0.144954\n",
+      "step: 9500 W: [[ 1.4979682 ]\n",
+      " [ 0.34225735]] b: [-5.71418619] 0.143905\n",
+      "step: 9600 W: [[ 1.50324261]\n",
+      " [ 0.34758565]] b: [-5.74544764] 0.142871\n",
+      "step: 9700 W: [[ 1.50849116]\n",
+      " [ 0.35286024]] b: [-5.77648449] 0.141852\n",
+      "step: 9800 W: [[ 1.51371431]\n",
+      " [ 0.35808238]] b: [-5.80729771] 0.140848\n",
+      "step: 9900 W: [[ 1.51891243]\n",
+      " [ 0.36325291]] b: [-5.83789396] 0.139858\n",
+      "step: 10000 W: [[ 1.52408516]\n",
+      " [ 0.36837265]] b: [-5.86827278] 0.138882\n",
+      "step: 10100 W: [[ 1.52923298]\n",
+      " [ 0.37344262]] b: [-5.89843988] 0.13792\n",
+      "step: 10200 W: [[ 1.534356 ]\n",
+      " [ 0.3784638]] b: [-5.92839527] 0.136971\n",
+      "step: 10300 W: [[ 1.53945446]\n",
+      " [ 0.38343689]] b: [-5.95814514] 0.136035\n",
+      "step: 10400 W: [[ 1.54452848]\n",
+      " [ 0.38836259]] b: [-5.98769045] 0.135112\n",
+      "step: 10500 W: [[ 1.54957807]\n",
+      " [ 0.393242  ]] b: [-6.01703215] 0.134202\n",
+      "step: 10600 W: [[ 1.55460322]\n",
+      " [ 0.39807582]] b: [-6.04617453] 0.133304\n",
+      "step: 10700 W: [[ 1.55960453]\n",
+      " [ 0.40286484]] b: [-6.0751214] 0.132418\n",
+      "step: 10800 W: [[ 1.56458211]\n",
+      " [ 0.40760976]] b: [-6.10387421] 0.131544\n",
+      "step: 10900 W: [[ 1.56953597]\n",
+      " [ 0.41231146]] b: [-6.1324358] 0.130681\n",
+      "step: 11000 W: [[ 1.57446647]\n",
+      " [ 0.41697055]] b: [-6.16080761] 0.12983\n",
+      "step: 11100 W: [[ 1.5793736]\n",
+      " [ 0.4215878]] b: [-6.18899298] 0.12899\n",
+      "step: 11200 W: [[ 1.5842576 ]\n",
+      " [ 0.42616382]] b: [-6.21699476] 0.128161\n",
+      "step: 11300 W: [[ 1.58911872]\n",
+      " [ 0.43069935]] b: [-6.24481487] 0.127343\n",
+      "step: 11400 W: [[ 1.5939573 ]\n",
+      " [ 0.43519503]] b: [-6.27245378] 0.126535\n",
+      "step: 11500 W: [[ 1.59877324]\n",
+      " [ 0.43965146]] b: [-6.29991531] 0.125738\n",
+      "step: 11600 W: [[ 1.60356688]\n",
+      " [ 0.44406942]] b: [-6.32720375] 0.124951\n",
+      "step: 11700 W: [[ 1.60833848]\n",
+      " [ 0.4484494 ]] b: [-6.35431671] 0.124174\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 11800 W: [[ 1.61308825]\n",
+      " [ 0.45279205]] b: [-6.38125992] 0.123406\n",
+      "step: 11900 W: [[ 1.61781538]\n",
+      " [ 0.45709786]] b: [-6.40803432] 0.122648\n",
+      "step: 12000 W: [[ 1.62252104]\n",
+      " [ 0.46136749]] b: [-6.43464231] 0.1219\n",
+      "step: 12100 W: [[ 1.62720501]\n",
+      " [ 0.4656015 ]] b: [-6.46108532] 0.121161\n",
+      "step: 12200 W: [[ 1.631868  ]\n",
+      " [ 0.46980056]] b: [-6.48736715] 0.120431\n",
+      "step: 12300 W: [[ 1.63651013]\n",
+      " [ 0.4739652 ]] b: [-6.51348543] 0.119709\n",
+      "step: 12400 W: [[ 1.64113021]\n",
+      " [ 0.47809574]] b: [-6.53944635] 0.118997\n",
+      "step: 12500 W: [[ 1.6457299 ]\n",
+      " [ 0.48219272]] b: [-6.56525183] 0.118293\n",
+      "step: 12600 W: [[ 1.65030897]\n",
+      " [ 0.48625684]] b: [-6.59089994] 0.117598\n",
+      "step: 12700 W: [[ 1.65486789]\n",
+      " [ 0.49028856]] b: [-6.61639547] 0.116911\n",
+      "step: 12800 W: [[ 1.65940523]\n",
+      " [ 0.49428836]] b: [-6.64173985] 0.116232\n",
+      "step: 12900 W: [[ 1.66392314]\n",
+      " [ 0.49825656]] b: [-6.66693497] 0.115561\n",
+      "step: 13000 W: [[ 1.66842151]\n",
+      " [ 0.50219369]] b: [-6.69198227] 0.114897\n",
+      "step: 13100 W: [[ 1.67289865]\n",
+      " [ 0.50610036]] b: [-6.71688318] 0.114242\n",
+      "step: 13200 W: [[ 1.67735648]\n",
+      " [ 0.5099768 ]] b: [-6.74163914] 0.113594\n",
+      "step: 13300 W: [[ 1.68179584]\n",
+      " [ 0.51382357]] b: [-6.76625252] 0.112954\n",
+      "step: 13400 W: [[ 1.68621397]\n",
+      " [ 0.51764101]] b: [-6.79072428] 0.112321\n",
+      "step: 13500 W: [[ 1.69061375]\n",
+      " [ 0.52142966]] b: [-6.81505632] 0.111695\n",
+      "step: 13600 W: [[ 1.69499445]\n",
+      " [ 0.52519   ]] b: [-6.83925009] 0.111076\n",
+      "step: 13700 W: [[ 1.69935548]\n",
+      " [ 0.52892202]] b: [-6.86330795] 0.110464\n",
+      "step: 13800 W: [[ 1.70369899]\n",
+      " [ 0.53262651]] b: [-6.88723135] 0.109859\n",
+      "step: 13900 W: [[ 1.70802212]\n",
+      " [ 0.53630376]] b: [-6.91102171] 0.109261\n",
+      "step: 14000 W: [[ 1.71232772]\n",
+      " [ 0.53995407]] b: [-6.93467712] 0.10867\n",
+      "step: 14100 W: [[ 1.71661413]\n",
+      " [ 0.54357809]] b: [-6.95820332] 0.108085\n",
+      "step: 14200 W: [[ 1.72088337]\n",
+      " [ 0.54717606]] b: [-6.98160219] 0.107506\n",
+      "step: 14300 W: [[ 1.72513354]\n",
+      " [ 0.55074799]] b: [-7.00487185] 0.106934\n",
+      "step: 14400 W: [[ 1.72936618]\n",
+      " [ 0.55429447]] b: [-7.02801371] 0.106368\n",
+      "step: 14500 W: [[ 1.73358059]\n",
+      " [ 0.55781597]] b: [-7.0510335] 0.105807\n",
+      "step: 14600 W: [[ 1.73777771]\n",
+      " [ 0.56131274]] b: [-7.07392645] 0.105253\n",
+      "step: 14700 W: [[ 1.74195683]\n",
+      " [ 0.56478506]] b: [-7.09669924] 0.104705\n",
+      "step: 14800 W: [[ 1.74611914]\n",
+      " [ 0.56823319]] b: [-7.11934996] 0.104163\n",
+      "step: 14900 W: [[ 1.75026309]\n",
+      " [ 0.5716576 ]] b: [-7.14188147] 0.103626\n",
+      "step: 15000 W: [[ 1.75439167]\n",
+      " [ 0.57505852]] b: [-7.16429377] 0.103095\n",
+      "step: 15100 W: [[ 1.75850081]\n",
+      " [ 0.5784362 ]] b: [-7.18659019] 0.10257\n",
+      "step: 15200 W: [[ 1.7625947 ]\n",
+      " [ 0.58179116]] b: [-7.20876837] 0.10205\n",
+      "step: 15300 W: [[ 1.7666713 ]\n",
+      " [ 0.58512348]] b: [-7.23083496] 0.101535\n",
+      "step: 15400 W: [[ 1.77073097]\n",
+      " [ 0.58843374]] b: [-7.2527833] 0.101026\n",
+      "step: 15500 W: [[ 1.77477551]\n",
+      " [ 0.59172153]] b: [-7.27462149] 0.100522\n",
+      "step: 15600 W: [[ 1.77880144]\n",
+      " [ 0.59498781]] b: [-7.29635] 0.100023\n",
+      "step: 15700 W: [[ 1.78281248]\n",
+      " [ 0.59823275]] b: [-7.31796455] 0.0995289\n",
+      "step: 15800 W: [[ 1.78680742]\n",
+      " [ 0.60145628]] b: [-7.33947182] 0.09904\n",
+      "step: 15900 W: [[ 1.79078531]\n",
+      " [ 0.60465932]] b: [-7.36087227] 0.0985559\n",
+      "step: 16000 W: [[ 1.79474831]\n",
+      " [ 0.60784155]] b: [-7.38216543] 0.0980767\n",
+      "step: 16100 W: [[ 1.79869545]\n",
+      " [ 0.61100328]] b: [-7.40335035] 0.0976022\n",
+      "step: 16200 W: [[ 1.80262589]\n",
+      " [ 0.61414498]] b: [-7.42443132] 0.0971324\n",
+      "step: 16300 W: [[ 1.80654144]\n",
+      " [ 0.6172666 ]] b: [-7.4454093] 0.0966672\n",
+      "step: 16400 W: [[ 1.81044269]\n",
+      " [ 0.62036872]] b: [-7.46628523] 0.0962065\n",
+      "step: 16500 W: [[ 1.81432664]\n",
+      " [ 0.62345147]] b: [-7.48705959] 0.0957504\n",
+      "step: 16600 W: [[ 1.81819606]\n",
+      " [ 0.62651503]] b: [-7.50773287] 0.0952986\n",
+      "step: 16700 W: [[ 1.82205093]\n",
+      " [ 0.62955946]] b: [-7.52830458] 0.0948513\n",
+      "step: 16800 W: [[ 1.82589114]\n",
+      " [ 0.63258505]] b: [-7.54877758] 0.0944081\n",
+      "step: 16900 W: [[ 1.82971489]\n",
+      " [ 0.63559222]] b: [-7.56915331] 0.0939692\n",
+      "step: 17000 W: [[ 1.83352458]\n",
+      " [ 0.63858128]] b: [-7.58943176] 0.0935346\n",
+      "step: 17100 W: [[ 1.83732021]\n",
+      " [ 0.64155209]] b: [-7.60961437] 0.0931039\n",
+      "step: 17200 W: [[ 1.84110141]\n",
+      " [ 0.64450508]] b: [-7.62970209] 0.0926774\n",
+      "step: 17300 W: [[ 1.84486735]\n",
+      " [ 0.64744037]] b: [-7.64969587] 0.0922548\n",
+      "step: 17400 W: [[ 1.84861863]\n",
+      " [ 0.65035826]] b: [-7.6695962] 0.0918362\n",
+      "step: 17500 W: [[ 1.85235631]\n",
+      " [ 0.65325892]] b: [-7.68940449] 0.0914214\n",
+      "step: 17600 W: [[ 1.85608006]\n",
+      " [ 0.65614265]] b: [-7.70912123] 0.0910104\n",
+      "step: 17700 W: [[ 1.85979009]\n",
+      " [ 0.65900934]] b: [-7.72874546] 0.0906033\n",
+      "step: 17800 W: [[ 1.86348581]\n",
+      " [ 0.66185921]] b: [-7.74828005] 0.0901999\n",
+      "step: 17900 W: [[ 1.867167  ]\n",
+      " [ 0.66469264]] b: [-7.76772547] 0.0898001\n",
+      "step: 18000 W: [[ 1.87083495]\n",
+      " [ 0.66750997]] b: [-7.78708315] 0.0894039\n",
+      "step: 18100 W: [[ 1.87448907]\n",
+      " [ 0.67031109]] b: [-7.80635405] 0.0890114\n",
+      "step: 18200 W: [[ 1.87813032]\n",
+      " [ 0.67309606]] b: [-7.82553959] 0.0886222\n",
+      "step: 18300 W: [[ 1.88175797]\n",
+      " [ 0.67586553]] b: [-7.84463739] 0.0882367\n",
+      "step: 18400 W: [[ 1.8853724 ]\n",
+      " [ 0.67861927]] b: [-7.86364794] 0.0878546\n",
+      "step: 18500 W: [[ 1.88897383]\n",
+      " [ 0.68135732]] b: [-7.88257599] 0.0874758\n",
+      "step: 18600 W: [[ 1.89256144]\n",
+      " [ 0.68408066]] b: [-7.90142202] 0.0871003\n",
+      "step: 18700 W: [[ 1.89613557]\n",
+      " [ 0.6867882 ]] b: [-7.92018461] 0.0867282\n",
+      "step: 18800 W: [[ 1.89969718]\n",
+      " [ 0.68948162]] b: [-7.93886232] 0.0863593\n",
+      "step: 18900 W: [[ 1.90324581]\n",
+      " [ 0.69215965]] b: [-7.9574604] 0.0859936\n",
+      "step: 19000 W: [[ 1.90678155]\n",
+      " [ 0.69482315]] b: [-7.97598076] 0.0856311\n",
+      "step: 19100 W: [[ 1.91030526]\n",
+      " [ 0.69747227]] b: [-7.99441528] 0.0852717\n",
+      "step: 19200 W: [[ 1.91381681]\n",
+      " [ 0.70010692]] b: [-8.01276875] 0.0849155\n",
+      "step: 19300 W: [[ 1.91731453]\n",
+      " [ 0.70272768]] b: [-8.03105164] 0.0845621\n",
+      "step: 19400 W: [[ 1.92079937]\n",
+      " [ 0.70533431]] b: [-8.04926014] 0.0842117\n",
+      "step: 19500 W: [[ 1.92427254]\n",
+      " [ 0.70792687]] b: [-8.06737995] 0.0838645\n",
+      "step: 19600 W: [[ 1.92773426]\n",
+      " [ 0.71050638]] b: [-8.08542061] 0.0835203\n",
+      "step: 19700 W: [[ 1.93118477]\n",
+      " [ 0.71307129]] b: [-8.10339069] 0.0831788\n",
+      "step: 19800 W: [[ 1.93462038]\n",
+      " [ 0.71562314]] b: [-8.12129021] 0.0828401\n",
+      "step: 19900 W: [[ 1.93804455]\n",
+      " [ 0.71816194]] b: [-8.13912106] 0.0825042\n",
+      "step: 20000 W: [[ 1.94145763]\n",
+      " [ 0.72068757]] b: [-8.1568594] 0.0821713\n",
+      "Accuracy: 1.0\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    \n",
+    "    for step in range(20001):\n",
+    "        cost_val, _ = sess.run([cost, train], feed_dict = {X : x_data, Y : y_data})\n",
+    "        if step % 100 == 0:\n",
+    "            print('step:', step, 'W:', sess.run(W), 'b:', sess.run(b), cost_val)\n",
+    "            \n",
+    "    print('Accuracy:', sess.run(accuracy, feed_dict = {X : x_data, Y : y_data}))"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# (번외 2) Optimizer 성능비교"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "- 변수가 1개 있는 linear regression에서 맛보기로 opimizer를 비교했었다.\n",
+    "- 비록 해당 모델에서는 아주 기본적인 GradientDescent optimizer가 가장 안정적으로 로스를 줄였지만, 실제 쓰이는 모델과 흡사한 이번 모델에서는 그렇지 않을 것이다!\n",
+    "- 최적의 결과를 위해 각 optimizer 별로 learing rate를 다르게 줬다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## GradientDescent "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 28,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 30,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: 2.09241 \n",
+      "predicting y:\n",
+      " [[ 0.02708844]\n",
+      " [ 0.00845576]\n",
+      " [ 0.10248329]\n",
+      " [ 0.00983038]\n",
+      " [ 0.01059847]\n",
+      " [ 0.03912268]]\n",
+      "step: 10000 cost: 1.72727 \n",
+      "predicting y:\n",
+      " [[ 0.03506354]\n",
+      " [ 0.01315404]\n",
+      " [ 0.15031098]\n",
+      " [ 0.01924227]\n",
+      " [ 0.02324985]\n",
+      " [ 0.08718664]]\n",
+      "step: 15000 cost: 1.39773 \n",
+      "predicting y:\n",
+      " [[ 0.04472765]\n",
+      " [ 0.01998303]\n",
+      " [ 0.2113566 ]\n",
+      " [ 0.03613175]\n",
+      " [ 0.04836842]\n",
+      " [ 0.17665954]]\n",
+      "step: 20000 cost: 1.12066 \n",
+      "predicting y:\n",
+      " [[ 0.05582392]\n",
+      " [ 0.02926515]\n",
+      " [ 0.2815741 ]\n",
+      " [ 0.06358242]\n",
+      " [ 0.09247702]\n",
+      " [ 0.31039327]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "기본템(?)답게 기대할 만한 결과가 나오지 않았다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Adam "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 31,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 32,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: nan \n",
+      "predicting y:\n",
+      " [[ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]]\n",
+      "step: 10000 cost: nan \n",
+      "predicting y:\n",
+      " [[ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]]\n",
+      "step: 15000 cost: nan \n",
+      "predicting y:\n",
+      " [[ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]]\n",
+      "step: 20000 cost: nan \n",
+      "predicting y:\n",
+      " [[ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]\n",
+      " [ nan]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "역시 널리 쓰이는 optimizer답게 가장 많이 cost를 낮추었다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Adadelta"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 33,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.1)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 34,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: 0.675026 \n",
+      "predicting y:\n",
+      " [[ 0.13855268]\n",
+      " [ 0.10209122]\n",
+      " [ 0.47627679]\n",
+      " [ 0.22261451]\n",
+      " [ 0.31246048]\n",
+      " [ 0.61819744]]\n",
+      "step: 10000 cost: 0.356612 \n",
+      "predicting y:\n",
+      " [[ 0.15172657]\n",
+      " [ 0.21842751]\n",
+      " [ 0.59868282]\n",
+      " [ 0.60747904]\n",
+      " [ 0.78457314]\n",
+      " [ 0.9280979 ]]\n",
+      "step: 15000 cost: 0.205572 \n",
+      "predicting y:\n",
+      " [[ 0.05637946]\n",
+      " [ 0.18039253]\n",
+      " [ 0.41297859]\n",
+      " [ 0.73097306]\n",
+      " [ 0.90518343]\n",
+      " [ 0.96969056]]\n",
+      "step: 20000 cost: 0.130054 \n",
+      "predicting y:\n",
+      " [[ 0.02335435]\n",
+      " [ 0.14957134]\n",
+      " [ 0.26627669]\n",
+      " [ 0.80304658]\n",
+      " [ 0.95153111]\n",
+      " [ 0.98409665]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "그렇게 좋지도 나쁘지도 않은 결과가 나왔다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Momentum"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.MomentumOptimizer(0.1e-4, 0.5)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: 1.72742 \n",
+      "predicting y:\n",
+      " [[ 0.03505907]\n",
+      " [ 0.01315139]\n",
+      " [ 0.15028727]\n",
+      " [ 0.01923682]\n",
+      " [ 0.02324235]\n",
+      " [ 0.08715944]]\n",
+      "step: 10000 cost: 1.12074 \n",
+      "predicting y:\n",
+      " [[ 0.05581899]\n",
+      " [ 0.02926107]\n",
+      " [ 0.28155011]\n",
+      " [ 0.06357088]\n",
+      " [ 0.09245867]\n",
+      " [ 0.31034639]]\n",
+      "step: 15000 cost: 0.751632 \n",
+      "predicting y:\n",
+      " [[ 0.07977711]\n",
+      " [ 0.05411958]\n",
+      " [ 0.420311  ]\n",
+      " [ 0.1516104 ]\n",
+      " [ 0.24001835]\n",
+      " [ 0.59913367]]\n",
+      "step: 20000 cost: 0.572923 \n",
+      "predicting y:\n",
+      " [[ 0.10189588]\n",
+      " [ 0.08234517]\n",
+      " [ 0.52549839]\n",
+      " [ 0.25954497]\n",
+      " [ 0.40925366]\n",
+      " [ 0.77383292]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "꽤 괜찮은 성적이다.\n",
+    "\n",
+    "그리고 또 한가지 놀라운 점은 cost가 3만대에서 소수점까지 떨어진 것이다.\n",
+    "\n",
+    "정말 무서운 속도를 보여준다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## RMSprop"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.RMSPropOptimizer(1e-3)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 38,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: 0.174099 \n",
+      "predicting y:\n",
+      " [[ 0.0433345 ]\n",
+      " [ 0.1791776 ]\n",
+      " [ 0.35236228]\n",
+      " [ 0.76560825]\n",
+      " [ 0.92666   ]\n",
+      " [ 0.97514457]]\n",
+      "step: 10000 cost: 0.055633 \n",
+      "predicting y:\n",
+      " [[ 0.00340072]\n",
+      " [ 0.0802547 ]\n",
+      " [ 0.11302604]\n",
+      " [ 0.89430887]\n",
+      " [ 0.98814106]\n",
+      " [ 0.99684525]]\n",
+      "step: 15000 cost: 0.0182071 \n",
+      "predicting y:\n",
+      " [[  2.23297524e-04]\n",
+      " [  2.91790757e-02]\n",
+      " [  3.65284234e-02]\n",
+      " [  9.60313916e-01]\n",
+      " [  9.98545647e-01]\n",
+      " [  9.99756634e-01]]\n",
+      "step: 20000 cost: 0.00600542 \n",
+      "predicting y:\n",
+      " [[  1.41285173e-05]\n",
+      " [  9.91558470e-03]\n",
+      " [  1.20172091e-02]\n",
+      " [  9.86311018e-01]\n",
+      " [  9.99836326e-01]\n",
+      " [  9.99983907e-01]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "Adam 다음으로 성적이 제일 좋다.\n",
+    "\n",
+    "Adadelta와 비슷하지만 훨씬 잘 나온다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Adagrad"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 39,
+   "metadata": {
+    "collapsed": true
+   },
+   "outputs": [],
+   "source": [
+    "optimizer = tf.train.AdagradOptimizer(0.75)\n",
+    "\n",
+    "train = optimizer.minimize(cost)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 40,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "step: 0 cost: 2.47863 \n",
+      "predicting y:\n",
+      " [[ 0.02073799]\n",
+      " [ 0.00536009]\n",
+      " [ 0.06783572]\n",
+      " [ 0.00490376]\n",
+      " [ 0.00469031]\n",
+      " [ 0.01665213]]\n",
+      "step: 5000 cost: 0.00511627 \n",
+      "predicting y:\n",
+      " [[  9.89229466e-06]\n",
+      " [  8.87341797e-03]\n",
+      " [  9.31381807e-03]\n",
+      " [  9.87800598e-01]\n",
+      " [  9.99870181e-01]\n",
+      " [  9.99987006e-01]]\n",
+      "step: 10000 cost: 0.00260144 \n",
+      "predicting y:\n",
+      " [[  1.82619885e-06]\n",
+      " [  4.53802664e-03]\n",
+      " [  4.73193172e-03]\n",
+      " [  9.93740499e-01]\n",
+      " [  9.99966264e-01]\n",
+      " [  9.99997616e-01]]\n",
+      "step: 15000 cost: 0.00174477 \n",
+      "predicting y:\n",
+      " [[  6.72978615e-07]\n",
+      " [  3.04931193e-03]\n",
+      " [  3.17308097e-03]\n",
+      " [  9.95789111e-01]\n",
+      " [  9.99984741e-01]\n",
+      " [  9.99999166e-01]]\n",
+      "step: 20000 cost: 0.00131271 \n",
+      "predicting y:\n",
+      " [[  3.30463422e-07]\n",
+      " [  2.29632063e-03]\n",
+      " [  2.38707289e-03]\n",
+      " [  9.96827066e-01]\n",
+      " [  9.99991417e-01]\n",
+      " [  9.99999523e-01]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "with tf.Session() as sess:\n",
+    "    sess.run(tf.global_variables_initializer())\n",
+    "    for step in range(20001):\n",
+    "        cost_val, hypo_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
+    "        if step%5000 == 0:\n",
+    "            print(\"step:\", step, \"cost:\", cost_val, \"\\npredicting y:\\n\", hypo_val)"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "선형 모델에 적합한 특징답게, 선형 모델에서 우수한 성적이 나온다.\n",
+    "\n",
+    "거의 Adam과 비슷하다."
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "한 가지 주의점은 optimizer별로 잘 되는 learing rate가 다르니 0.1씩이나 0.05씩 값을 조정해나가면서 찾는 게 좋은 것 같다."
+   ]
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.6.1"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
